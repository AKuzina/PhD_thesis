\section{Related work}

\textbf{DDGM for image generation} 
Various modifications of DDGMs were recently proposed to improve their sampling quality. This includes simplifying the learning objective and proposing new noise schedulers, which allow DDGMs to achieve state-of-the-art results. In this work, we show that splitting the decoder into two parts, namely, a denoiser and a generator, can benefit the performance, especially when training with the variational lower bound.

\textbf{Properties of DDGMs} 
In~\citet{ho2020denoising} authors notice that DDGMs can be beneficial for lossy compression, observing (Figure 5 in~\citet{ho2020denoising}) that most of the bits are allocated to the region of the smallest distortion that corresponds to the first steps of a DDGM. 
We draw a similar conclusion when discussing the denoising ability of the diffusion model in Section \ref{sec:analysis}. However, we base our analysis on the signal-to-noise ratio rather than compression. On the other hand~\citet{salimans2022progressive} focus on the computational complexity of DDGM and propose a progressive distillation that iteratively reduces the number of diffusion steps. The work shows that it is possible to considerably reduce the number of sampling steps without losing performance. We believe that their results support our intuition that it is reasonable to combine several initial steps into a single denoiser model. In~\cite{benny2022dynamic}, authors evaluate how the diffusion process changes in time when model is trained with different objectives (Eq.~\ref{eq:l_t} or Eq.~\ref{eq:l_t_simple}). They observe that the image generation process differs significantly and that it is more beneficial to switch between those two approaches at different stages of the diffusion. 
In this work, we also investigate changes in the diffusion process, but we focus on the generative and denoising capabilities of the model instead.

\textbf{Connection to hierarchical Variational Autoencoders} Several works have noted the connection of DDGM to VAEs. In~\citet{huang2021variational} authors focus on the continuous diffusion models and draw the connection to the infinitely deep hierarchical VAEs. In~\citet{kingma2021variational} authors further explore this connection, formulate a VLB objective in terms of the signal-to-noise ratio and propose to learn noise schedule, which brings the forward diffusion process even closer to the encoder of a VAE. 
Recently a latent score-based generative model (LSGM) was proposed~\cite{vahdat2021score}, which can be seen as a VAE with the score-based prior. We follow a similar direction and propose to see a DDGM as a combination of a denoising auto-encoder with an additional diffusion-based generator of corrupted images.
