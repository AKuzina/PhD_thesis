



\chapter[On Analyzing Generative and Denoising Capabilities of DDGMs]{On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models}\label{chap:daed}

\begin{quote}
\normalsize\itshape
\begin{flushright}
% \foreignlanguage{russian}{- "Я стол накрыл на шестерых"...}\\
% \foreignlanguage{russian}{Ты одного забыл - седьмого.}  \\
% \foreignlanguage{russian}{Марина Цветаева} \\ \vskip 10pt
 % And the questions...  The questions lack answers, still missing:\\
 % They'll come and they'll burn, fade like measles, unkind.\\
 % Sasha Chorny
\end{flushright}
\end{quote}



\begin{flushright}
	\small{
		\textit{
			\hfill This chapter is based on the NeurIPS 2022 paper \citep{deja2022analyzing} 
		} 
		
	}
\end{flushright}

\paragraph{Abstract}

Diffusion-based Deep Generative Models (DDGMs) offer state-of-the-art performance in generative modeling. Their main strength comes from their unique setup in which a model (the backward diffusion process) is trained to reverse the forward diffusion process, which gradually adds noise to the input signal. Although DDGMs are well studied, it is still unclear how the small amount of noise is transformed during the backward diffusion process. 
Here, we focus on analyzing this problem to gain more insight into the behavior of DDGMs and their denoising and generative capabilities. We observe a fluid transition point that changes the functionality of the backward diffusion process from generating a (corrupted) image from noise to denoising the corrupted image to the final sample. Based on this observation, we postulate to divide a DDGM into two parts: a denoiser and a generator. The denoiser could be parameterized by a denoising auto-encoder, while the generator is a diffusion-based model with its own set of parameters. We experimentally validate our proposition, showing its pros and cons.

\newpage
% = SECTION =
\input{text/4_daed/sections/0_intro}

% = SECTION =
\input{text/4_daed/sections/1_background}

% = SECTION =
\input{text/4_daed/sections/2_method}

% = SECTION =
\input{text/4_daed/sections/3_related}

% = SECTION =
\input{text/4_daed/sections/4_experiments}

% = SECTION =

\section{Conclusion}
In this work, we investigate the generative and denoising capabilities of the Diffusion-based Deep Generative Models. We observe and experimentally validate that it is reasonable to understand DDGMs as a combination of two parts. The first one generates noisy samples from the pure noise by inputting more signal from a learned data distribution, while the second one removes the remaining noise from the signal. Although for standard DDGMs, the exact switching point between those two parts is fluid, we propose a new approach dubbed \ours{} that is explicitly built as a combination of a generative component (a DDGM) and a denoising one (a DAE). 
In the experiments, we observe that \ours{} simplifies training with a standard VLB loss function that leads to improved performance. On the other hand, with increasing noise processed by DAE, \ours{} smoothens the generations resulting in lower performance when training with the simplified objective. 
We further show that DDGMs, and \ours{} especially, generalize well to unseen data, what opens new possibilities for further research in terms of transfer or continual learning of DDGMs.

