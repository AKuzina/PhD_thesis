\chapter{Introduction and Background}

\begin{quote}
\normalsize\itshape
\begin{flushright}
\foreignlanguage{russian}{А вопросы… Вопросы не знают ответа —}\\
\foreignlanguage{russian}{Налетят, разожгут и умчатся, как корь.} \\
\foreignlanguage{russian}{Саша Черный} \\ \vskip 10pt
 And the questions...  The questions lack answers, still missing:\\
 They'll come and they'll burn, fade like measles, unkind.\\
 Sasha Chorny
\end{flushright}
\end{quote}

% \begin{verse}
% \normalsize\itshape
% \hfill\foreignlanguage{russian}{- "Я стол накрыл на шестерых"...}\\
% \hfill\foreignlanguage{russian}{Ты одного забыл - седьмого.} \\
% \hfill\foreignlanguage{russian}{М. Цветаева} \\ \vskip 15pt
% \end{verse}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Gneral Introduction, some talkson how deep generative mdoels are cool goes here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
In recent years, deep learning has seen significant advances, particularly in the field of generative models. Two of the most successful applications of Deep Generative Models (DGMs) are natural language processing and computer vision. In natural language processing, Large Language Models (LLMs), which usually constitute autoregressive generative models~\citep{graves2013generating} with a transformer-based architecture~\citep{vaswani2017attention}, have demonstrated impressive text generation capabilities~\citep{brown2020language,chowdhery2023palm}. They have produced long and coherent texts across various contexts and gained popularity in the form of chat bots~\citep{achiam2023gpt}. In computer vision, diffusion models are achieving impressive results in generating photorealistic images~\citep{dhariwal2021diffusion} and videos~\citep{ho2022video}.

% Bayesian inference and human learning \cite{xu2007word}

Practically, modeling the probability distribution of the data offers two main functionalities to the underlying model: sampling and likelihood estimates. A lot of attention is paid to the sampling functionality, as it can be used to produce completely novel datapoints, e.g. images that look like real ones but were not observed by the model during training. Furthermore, conditional sampling from the distribution can be used to provide forecast or prediction in the situation when only part of the variables is observed. Access to the likelihood, if present, might also offer many benefits. This includes, but is not limited to, the estimation of uncertainty and the detection of out-of-distribution \cite{havtorn2021hierarchical, kadavath2022language}, as well as handling the missing data \cite{mattei2019miwae} and anomalies \citep{an2015variational}.

Despite enormous successes, many challenges still remain in the field of deep generative modeling~\citep{manduchi2024challenges}. The present thesis focuses on one type of generative models, namely, Latent Variable Generative Models (LVMs). We consider two general aspects in which deep LVMs can be improved. 
The first aspect is presented in Part~\ref{part:1} of the thesis and it concerns the choise of the probabilistic model that improves the density estimation performance of the model. 
% and it concerns the choice of the prior distribution and the way it affects model performance in the different scenarios. 
The second aspect covered in Part~\ref{part:2} focuses on the various properties of the resulting latent representations, which can be especially important for the downstream applications of the generative models. 

In this chapter, we discuss all the background crucial for understanding the contributions of the thesis and present the research questions. We formalize the generative modeling task and discuss the variety of model types based on the different probabilistic assumptions in Section~\ref{sec:intro_generative_models_types}, talk about neural network parametrization of probabilistic models in Section~\ref{sec:intro_parametrization} and discuss learning algorithms used to fit unknown parameters to the data in Section~\ref{sec:intro_learning_algo}. Finally, we take a closer look at the Latent Variable Generative models in Section~\ref{sec:into_latent_variable_models}, where we cover open question and challenges and formulate the research questions addressed in this thesis.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Main Seciton: Generative models Definition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Deep Generative Modeling}\label{sec:intro_generative_models_types}
We formulate Deep Generative Modeling as a problem of density estimation and deep neural network parametrization. 
The task is to model a distribution of the random variable $\rvx$ given a finite set of \textit{independent and identically distributed} (iid) random samples $\{x_n\}_{n=1}^N$. 
We will denote this finite set of samples as an empirical data distribution $p_e(\rvx) = \frac1N \sum_n \delta\left(\rvx - x_n\right)$. 
There are three components of the model that need to be specified:
\begin{itemize}
\item Probabilistic model. \newline 
\marginnote{We use $p_{\theta}(\rvx)$ to denote probability density function of random variable $\rvx$, which is defined by (possibly unknown) parameters $\theta$.}
The probabilistic model $p_{\theta}(\rvx)$ formalizes the assumption about the generative process of the observed data. 
We seek to have flexible and expressive model to capture complex dependencies in the observed data, while as the same time keeping the method scalable.


\item Parametrization.\\
In deep generative models, the probabilistic model from above is parameterized by the deep neural network. 
The choice of architecture depends on the type of data and should respect any constraints implied by the probabilistic framework (e.g. we expect variance of the distribution to be always positive).

\item Learning algorithm. \\
Finally, one has to choose how to tune unknown parameters $\theta$. The goal is to make sure that $p_{\theta}(\rvx)$ approximates a true data distribution for which only a finite number of samples is observed. For this purpose, we will formulate learning as an optimization problem and solve it using gradient-based methods. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Types of generative models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\subsection{Generative Models Zoo}
Deep generative models are usually differentiated by the probabilistic model. 
There are multiple ways in which these models can be grouped. In this chapter, we chose to distinguish the models with prescribed and implicit density \citep{diggle1984monte}. In the former case, the density function can be evaluated at least up to a normalization constant, while in the latter case only the sampling procedure from the distribution is defined. 
In Figure \ref{fig:into_model_types} we schematically present the dichotomy with the examples of modern deep generative models given for each type.

This thesis contributes to the field of prescribed density models with latent variables. To put the work in a more broad context, we will now discuss different types of prescribed density generative models.

\begin{figure}[t]
\begin{tikzpicture}[
node distance=1.5cm and 2cm,
box/.style={rectangle, draw, rounded corners, text centered, text width=4cm, minimum height=1cm, fill=RoyalBlue!10},
example/.style={rectangle, draw=none, text centered, rounded corners, text width=4cm, minimum height=0.5cm,fill=ForestGreen!10},
arrow/.style={->, thick, >=Stealth}
]

% Main block at the top center
\node[box, fill=RedOrange!20] (dgms) {Generative Model};

% Subcategories in one horizontal line below the main block
\node[box, below=of dgms, xshift=-3cm, text width=3.5cm] (full) {Prescribed density};
\node[box, below=of dgms, xshift=3cm, text width=3.5cm] (implicit) {Implicit density\\ $\tilde{\rvx} \sim p_{\theta}(\rvx)$};
\node[example, below=0.1cm of implicit, text width=3.5cm] (impl-x) {GAN};
%% Arrows connecting main block to subcategories
\draw[arrow] (dgms) -- (implicit);
\draw[arrow] (dgms) -- (full);


%\node[example, below=0cm of full, text width=4cm] (full-x) {Normalizing Flows};
\node[box, below=of full, xshift=-2cm, text width=3.5cm] (tractable) {Fully Tractable \\ $p_{\theta}(\rvx)$};
\node[box, below=of full, xshift=2cm, text width=3.5cm] (norm) {Unnormalized \\ $ p_{\theta}(\rvx) \propto f_{\theta}(\rvx) $};
\node[box, below=of full, xshift=6.2cm, text width=3.9cm] (aux) {Latent Variables \\ $p_{\theta}(\rvx) = \int p_{\theta}(\rvx| \rvz)p_{\theta}(\rvz)d\rvz$};
\draw[arrow] (full) -- (tractable);
\draw[arrow] (full) -- (norm);
\draw[arrow] (full) -- (aux);

\node[example, below=0.1cm of tractable, text width=3.5cm] (tractable-x) {Normalizing Flow};
\node[example, below=0.1cm of norm, text width=3.5cm] (norm-x) {Energy-based Model};
\node[example, below=0.1cm of aux, text width=3.9cm] (aux-x) {Variational Autoencoder};

%\node[box, below=of aux, xshift=-2cm, text width=3.5cm] (latent) {Latent \\ $ p(\rvz) $};
%\node[box, below=of aux, xshift=2cm, text width=3.5cm] (observed) {Non-latent \\ $q(\rvz|\rvx){}$};
%\draw[arrow] (aux) -- (latent);
%\draw[arrow] (aux) -- (observed);

\end{tikzpicture}
\caption{
Different ways to model a probability distribution of the observed random variable $\rvx$ with examples of generative models.
% Different types of generative models based on the underlying probabilistic model.
}
\label{fig:into_model_types}
\end{figure}


\subsection{Fully Tractable Likelihood}
As the name suggests, this class of models provides direct access to the likelihood, without any approximation needed. This is beneficial for tasks where density estimation is the main goal.

\marginnote[1.5\baselineskip]{\citet{papamakarios2021normalizing} provide a comprehensive review and details on how to construct normalizing flows.} 
\textit{Normalizing flow} is an example of a generative model with fully tractable likelihood. Here, the distribution of interest is produced by pushing a simple base distribution $\pi(\cdot)$ through a bijective transformation $f_{\theta}(\cdot)$. This allows to compute the likelihood exactly using change of variables formula:
\marginnote[0.5\baselineskip]{Here, $\mJ_{f_{\theta}}$ denotes the Jacobian of the function $f_{\theta}$.}
\begin{equation}\label{eq:intro_nf}
    p_{\theta}(\rvx) = \pi\left(f^{-1}_{\theta}(\rvx)\right) \left|\text{det}\mJ_{f_{\theta}}  \right|^{-1}.
\end{equation}
These models allow for fast sampling and exact likelihood calculations. However, the bijectivity requirement introduces a constraint on the parameterization and may influence the expressivity of the model. 

Another way to define a fully tractable probabilistic model over a high-dimensional input $\rvx$ is to use the product rule of probability, which gives rise to \textit{Autoregressive Models} (ARMs).  
\marginnote[\baselineskip]{We denote $\rvx_{<i} = (\rvx_1, \dots, \rvx_{i-1})$ and assume $\rvx_{<1} = \varnothing$. }
\begin{equation}\label{eq:intro_arm}
    p_{\theta}(\rvx) = \prod_{i}p_{\theta}(\rvx_i | \rvx_{<i}),
\end{equation}
Usually, all conditional distributions in Eq.~\ref{eq:intro_arm} are parameterized by a shared model, and likelihood can be efficiently computed by evaluating all terms in parallel. The bottleneck of this model is the speed of sampling, which can only be done sequentially, one dimension at a time.

\subsection{Unnormalized Likelihood}
The most flexible prescribed density generative models define the distribution up to an unknown normalization constant. 
This is achieved via energy function in the \textit{Energy-based models} (EBMs): 
\begin{equation}
    \log p_{\theta}(\rvx) = -f_{\theta}(\rvx) + \text{const},
\end{equation}
or, alternatively, via a score function in \textit{Score-based models}:
\begin{equation}\label{eq:intro_score}
    \nabla_{\rvx} p_{\theta}(\rvx) = f_{\theta}(\rvx).
\end{equation}
In both cases, one can use Markov Chain Monte Carlo (MCMC) techniques to sample from the resulting generative model.   Furthermore, it is not possible to obtain exact likelihood values because of the unknown normalization constant. 



\subsection{Latent variables}
This class of models introduces additional random variables (usually denoted $\rvz$) into a probabilistic model and assumes that the joint distribution $p_{\theta}(\rvx, \rvz) = p_{\theta}(\rvx| \rvz)p_{\theta}(\rvz)$ is fully tractable. These auxiliary variables can be treated merely as a way to increase the expressivity of the model, or one may expect $\rvz$ to capture underlying structure of the data. 

There are fewer restrictions on the parameterization of the model compared to, e.g., normalizing flows, and, at the same time, a fast sampling procedure is defined. However, the exact likelihood is not tractable. 
\begin{equation}\label{eq:intro_lvm_def}
     p_{\theta}(\rvx) = \int  p_{\theta}(\rvx| \rvz)p_{\theta}(\rvz) d\rvz
\end{equation}

When the auxiliary variables are treated as fully unobserved, we call them \textit{latent} and refer to the corresponding generative model as \textit{latent variable model}. 
Variational Autoencoder (VAE) \citep{kingma2014autoencoding, rezende2014stochastic} is a popular neural network parameterized latent variable model. 
\marginnote[-10pt]{Some generative models are hard to assign to a single category. For example, score-base diffusion models \cite{song2020score} introduce \textit{auxiliary} "noisy" random variables indexed by time and use the \textit{score} function to parameterize their distributions.}
In some cases, more assumptions about $\rvz$'s are made. For example, in Diffusion-based Generative Models (DGMs)~\citep{sohl2015deep, ho2020denoising} auxiliary variables are defined as noisy versions of the input $\rvx$. Despite the demonstrated correspondence between latent variables and the forward diffusion process \cite{huang2021variational, kingma2021variational, tzen2019neural} in DGMs, we find the term \textit{auxiliary} more suitable for this class of models. 


In Section \ref{sec:into_latent_variable_models}, we discuss all components of generative models with latent variables in more detail, as well as the research questions addressed in this thesis.

% The combination of latent and auxiliary variables \todo{iVAE and maybe some versions of learnable diffusion}

% The three most popular approaches to generative models are: generative adversarial networks (GAN) (Goodfellow et al., 2014), autoregressive models such as
% the PixelRNN (van den Oord et al., 2016b), and probabilistic deep generative
% models such as the variational auto-encoder (VAE) (Kingma and Welling, 2014;
% Rezende et al., 2014).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Architecture
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Parametrization}\label{sec:intro_parametrization}
Deep generative models have a wide range of applications, from image generation to material design. The key difference between these models and earlier generative models in machine learning is that deep generative models use neural networks for parameterization. 
For example, an invertible transformation in the Normalizing Flows or Score function in Score-based model is usually a specifically parametrized Neural Network, which we denoted as $f_{\theta}$ in Eq.~\ref{eq:intro_nf} and Eq.~\ref{eq:intro_score}. Due to the flexibility of neural networks, this allows us to obtain very flexible probability distributions. 

In autoregressive and latent variable models, on the other hand, we usually assume "easy to work with" conditional distributions, like Gaussian. However, we use neural networks to map the conditions to the parameters of these distributions. 
For example, consider a conditional distribution $p_{\theta}(\rvx|\rvz)$. Here, we would usually assume that there is a function $f_{\theta}$, which maps $\rvz$ (e.g., a realization of the random variable $\rvz$) into parameters of the probability distribution $p_{\theta}(\rvx|\rvz)$. For example, if $\rvx$ is a continuous random variable with infinite support, one can use Gaussian distribution:
\begin{equation}
    p_{\theta}(\rvx|\rvz) = \mathcal{N}\left(\rvx| \mu_{\theta}(\rvz), \sigma^2_{\theta}(\rvz) \text{I}\right),
\end{equation}
where both $\mu_{\theta}(\cdot)$ and $\sigma^2_{\theta}(\cdot)$ are Neural Networks (NNs) and $\theta$ are the trainable parameters of these NNs. 
% Other conditional distributions that are commonly used are Bernoulli, Discretized Logistic and Categorical. 

The specific application and type of generative model determine the required inductive biases and, consequently, the appropriate model architecture.
In this thesis, we focus on computer vision applications. We often use convolutional neural networks (CNNs)~\citep{krizhevsky2012imagenet}, such as ResNet~\citep{he2016deep} and U-Net~\citep{ronneberger2015u} in this domain. 
These architectures were initially developed for discriminative computer vision tasks such as classification, but have since been widely used to parameterize deep generative models. An alternative approach is to treat images as a sequence of patches and use transformer architecture~\citep{dosovitskiy2021an} as the backbone.

Applications of deep generative models are not limited to computer vision. Other domains include sequence modeling, such as texts, audio, or even DNA sequences. Here, transformers~\citep{vaswani2017attention, hoffmann2022empirical} and state space models~\citep{gu2021combining, nguyen2024hyenadna} are often employed. 
In scientific applications, such as molecule generation, graph neural networks~\citep{bruna2013spectral, kipf2016semi} (GNNs) are common. Molecules are often represented as point clouds embedded in 3D Euclidean space, and are modeled using Equivariant Graph Neural Networks (GNNs)~\citep{kohler2020equivariant, satorras2021n}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Inference methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Learning Algorithm}\label{sec:intro_learning_algo}
The learning algorithm is the final component of the deep generative model formulation. 
It involves setting up a loss function that measures how well a given model fits the data.
We use first-order gradient-based optimization methods to minimize the loss function. 
Therefore, we require the loss function to be differentiable with respect to the unknown parameters, allowing us to use backpropagation~\citep{rumelhart1986learning} to compute the gradients.

In this section, we first discuss the gradient-based optimization method used in deep learning and throughout this thesis. 
Next, we derive the objective functions used to train the generative models.

\subsection{Stochastic Gradient Descent}
Consider a loss function $\mathcal{L}(\rvx, \theta)$. 
\marginnote[-0.1\baselineskip]{A function that should be minimized with respect to model parameters will be called \textit{loss function}, \textit{cost function} and \textit{objective} interchangeably.}
Assume also that we can compute the gradient $\nabla_\theta \mathcal{L}(\rvx, \theta)$. Note that this objective is defined for a single datapoint, while we observe the dataset that contains $N$ independent points. Therefore the total loss that we want to minimize is the following:
\begin{equation}\label{eq:intro_loss}
\E_{p_e(\rvx)} \mathcal{L}(\rvx, \theta) = \frac1N \sum_n \mathcal{L}(\rvx_n, \theta) \rightarrow \min_{\theta}.
\end{equation}

We can then apply \textit{gradient descent}, a first-order iterative optimization algorithm. Under certain mild conditions on the loss function (e.g. convexity), this algorithm converges to the optimum. At each iteration of the method, we make a step in the direction of the fastest decrease of the loss function. We will also adjust the step size using a hyperparameter called \textit{learning rate}, that we will denote as $\eta_t$.
\begin{equation}\label{eq:intro_gd}
    \theta^{t+1} = \theta^{t} - \eta_{t} \frac1N \sum_n \nabla_{\theta}\mathcal{L}(\rvx_n, \theta^{t}).
\end{equation}

However, for large datasets, this method can be quite expensive to use, as the complexity of the gradient computation scales linearly with the number of points in the dataset. Therefore, the \textit{stochastic} version of the gradient descent is used. The idea is to use Monte Carlo estimate of the loss function (Eq.~\ref{eq:intro_loss}) when computing a gradient:
\begin{equation}
\E_{p_e(\rvx)} \mathcal{L}(\rvx, \theta) \approx \frac1M \sum_m \mathcal{L}(\rvx_, \theta), \quad M << N.
\end{equation}

In other words, we use a small subset of the dataset at each iteration. We refer to this subset as \textit{mini-batch}. We then compute a noisy gradient using only this mini-batch and use it to update the model parameters. The resulting algorithm, called \textit{stochastic gradient descent} (SGD) is presented in Algorithm~\ref{alg:sgd_train}. The algorithm is guaranteed to converge to a local minima for a certain learning rate schedule~\citep{robbins1951stochastic}.

\begin{algorithm}
	\caption{Training a Model with SGD}
	\label{alg:sgd_train}
	\begin{algorithmic}
  \\\hrulefill
\State \hskip-3mm  {\bfseries Input:} { Dataset $\mathcal{D}: \,\{x_n\}_{n=1}^N$}
\State \hskip-3mm  {\bfseries Input:} { Loss function $\mathcal{L}(\rvx, \theta)$}
		\While{not converged}
            \State Sample a mini-batch $\mathcal{M} \subset \mathcal{D}$
            \State Compute loss: $ \tilde{\mathcal{L}} = \frac1M \sum_{\rvx_i \in \mathcal{M}} \mathcal{L}(\rvx_i, \theta)$
		\State Compute gradient: $\hat{g}_t = \frac1M \sum_{\rvx_i \in \mathcal{M}} \nabla_{\theta} \mathcal{L}(\rvx_i, \theta^t)$
		\State Update parameters: $\theta^{t+1} = \theta^{t} - \eta_t \hat{g}_t $
		\EndWhile
            \State \hskip-3mm  {\bfseries Output:} $\theta^*$
	\end{algorithmic}
\end{algorithm}
\marginnote[-10\baselineskip]{We consider standard SGD update rule. Other first-order optimization methods (e.g. Adam~\citep{kingma2015adam}) use momentum to adjust the noisy gradient and second momentum to scale the learning rate for each parameter.}

\subsection{Maximum Likelihood}
We will now derive objective functions that are used to train deep generative models. We often want to minimize a divergence between the model and the empirical data distribution:

\begin{equation}\label{eq:intro_divergence}
\begin{aligned}
\text{D}\left(p_{e}(\rvx), p_{\theta}(\rvx)\right) \rightarrow \min_{\theta}
\end{aligned}
\end{equation} 

\marginnote[\baselineskip]{KL divergence is an example of $f$-Divergence that measures the distance between two distributions. Examples of other $f$-divergences are Jensen-Shannon Divergence and Total Variation distance. Other types of distance are the integral probability metric, of which a popular one is the Wasserstein distance.}
By definition, divergence is non-negative and equals zero if and only if two distributions are the same. 
A very common divergence used in generative modeling is the Kulbak-Leibler divergence (KL divergence). KL divergence between two distribution is defined as follows:
\begin{equation}
\begin{aligned} \label{eq:into_kl_def}
 \KL{p(\rvx)}{q(\rvx)} = \E_{p(\rvx)} \log \frac{p(\rvx)}{q(\rvx)}.
\end{aligned}
\end{equation}

Since it is not symmetric, it is convenient to distinguish forward and reverse KL divergences. We say that KL divergence is forward when the expectation in Eq.~\ref{eq:into_kl_def} is taken with respect to the \textit{target} distribution (which is an empirical data distribution in our case). Minimizing forward KL divergence between empirical distribution and a generative model corresponds to a widely used maximum likelihood objective:
\marginnote[\baselineskip]{Here we use definition of KL divergence and the fact that entropy of the empirical data distribution does not depend on the parameters $\theta$.}
\begin{equation}
\begin{aligned}
\KL{p_{e}(\rvx)}{p_{\theta}(\rvx)} & =  \E_{p_{e}(\rvx)}\left[ \log p_{e}(\rvx) - \log p_{\theta}(\rvx)\right] \\
& =  - \E_{p_{e}(\rvx)} \log p_{\theta}(\rvx) + \text{const}=\\
& =  - \tfrac1N \sum_n \underbrace{\log p_{\theta}(\rvx_n)}_{\text{Log-likelihood }}  + \text{ const}.
% & = \mathcal{L}^{ML}(\theta),
\end{aligned}
\end{equation}
Thus, loss function corresponding to maximum likelihood is the following:
\begin{equation}
    \mathcal{L}(\rvx, \theta) = -\log p_{\theta}(\rvx)
\end{equation}

Nevertheless, this approach is only directly applicable to models with fully tractable densities, e.g. normalizing flows and ARMs. For generative models with unnormalized density or unobserved latent variables, other approaches are used. 
We will now briefly discuss the most popular ones and show how they are connected to maximum likelihood.


\subsection{Score Matching}
Score matching~\citep{hyvarinen2005estimation} was proposed as a way to train EBMs and was later applied to Score-based generative models~\citep{song2019generative}. The idea is to use Fisher divergence instead of KL divergence in Eq.~\ref{eq:intro_divergence}:
\begin{equation}\label{eq:score_divergence}
\begin{aligned}
D_{\text{F}}\left[p_{e}(\rvx)||p_{\theta}(\rvx)\right]  =  \E_{p_{e}(\rvx)}\|\nabla_{\rvx} \log p_{e}(\rvx) - \nabla_{\rvx}\log p_{\theta}(\rvx)\|^2
\end{aligned}
\end{equation}

In EBMs, the model score $\nabla_{\rvx}\log p_{\theta}(\rvx)$ can be computed as a gradient of the energy function via backpropagation, while in the score-based generative models it is parametrized directly. However, the score of the empirical data distribution is not known. 
\citet{hyvarinen2005estimation} showed that the Fisher divergence can be further expressed without the intractable term $\nabla_{\rvx} \log p_{e}(\rvx)$: 
% \begin{fullwidth}
\begin{align}
\E_{p_{e}(\rvx)}&\|\nabla_{\rvx} \log p_{e}(\rvx) - \nabla_{\rvx}\log p_{\theta}(\rvx)\|^2 \notag \\
 = \int& p_e(\rvx) \Big(\|\nabla_{\rvx}\log p_{\theta}(\rvx)\|^2 - 2 \frac{\nabla_{\rvx}p_{e}(\rvx)^T}{p_{e}(\rvx)} \nabla_{\rvx}\log p_{\theta}(\rvx)  + \|\nabla_{\rvx} \log p_{e}(\rvx)\|^2 \Big)d\rvx \notag \\
= \int& p_e(\rvx) \|\nabla_{\rvx}\log p_{\theta}(\rvx)\|^2d\rvx 
    - 2 \int \nabla_{\rvx}p_{e}(\rvx)^T \nabla_{\rvx}\log p_{\theta}(\rvx)d\rvx  + \text{const}  \label{eq:sm_2}\\
= \int & p_e(\rvx) \|\nabla_{\rvx}\log p_{\theta}(\rvx)\|^2d\rvx 
    + 2 \int p_{e}(\rvx) \text{Tr}\left(\nabla^2_{\rvx}\log p_{\theta}(\rvx)\right)d\rvx  + \text{const} \label{eq:sm_3} \\
= \,\,&\,\,\E_{p_e(\rvx)} \Big[ \|\nabla_{\rvx}\log p_{\theta}(\rvx)\|^2 
    + 2 \text{Tr}\left(\nabla^2_{\rvx}\log p_{\theta}(\rvx)\right)\Big]  + \text{const}. \label{eq:sm_4}     
\end{align}
% \end{fullwidth}
\marginnote[-7\baselineskip]{Eq.~\ref{eq:sm_2} is the result of opening the brackets and noticing that the last term does not depend on the parameters $\theta$ and the integration by parts is applied to the second term of Eq.~\ref{eq:sm_2} to obtain Eq.~\ref{eq:sm_3}. }
As a result, we obtain the following \textit{score matching} objective:
\begin{equation}\label{eq:score_matching}
\begin{aligned}
\mathcal{L}^{SM}(\rvx, \theta)   =  \|\nabla_{\rvx}\log p_{\theta}(\rvx)\|^2 + 2\text{Tr}\left(\nabla^2_{\rvx}\log p_{\theta}(\rvx_n)\right).
\end{aligned}
\end{equation}
The connection between maximum likelihood and score matching was further explored through the connection between Fisher and Kulback-Leibler divergences~\cite{lyu2009interpretation}.

\subsection{Variational Inference}\label{sec:intro_vi}
Latent Variable Models also cannot be trained directly with the maximum likelihood or score matching approaches due to the intractability of marginal likelihood (Eq.~\ref{eq:intro_lvm_def}) or posterior distribution. \marginnote[-3\baselineskip]{The score of the latent variable model can be computed as follows:
$\nabla_{\rvx}\log p_{\theta}(\rvx) = \mathbb{E}_{p_{\theta}(\rvz|\rvx)} \nabla_{\rvx}\log p_{\theta}(\rvx, \rvz)$.
However, in most cases, the posterior $p_{\theta}(\rvz|\rvx)$ is not tractable.}
Note that the approximations of intractable posterior and marginal likelihood are two connected tasks as the two quantities are related:
\begin{equation}
    p_{\theta}(\rvz|\rvx) = \frac{p_{\theta}(\rvz, \rvx)}{p_{\theta}(\rvx)}
\end{equation}

Variational inference~\citep{jordan1999introduction} is the widely used method to train Deep Latent Variable models. 
The Markov chain Monte Carlo~\citep{neal1993probabilistic} is another way to approximate the unknown posterior distribution. 
We will now focus on the former method, derive the objective to learn both approximate posterior and unknown parameters $\theta$ and show how it is connected to maximum likelihood. 


Consider any probability distribution over the latent variable $q(\rvz)$. We can use it to obtain a lower bound on the log-likelihood objective function:
\marginnote[2\baselineskip]{We multiply the integrand by one in Eq.~\ref{eq:intro_elbo_def_2} and apply Jensen's inequality in Eq.~\ref{eq:intro_elbo_def_4}.  Jensen's inequality states that for a random variable $\rvx$ and a convex function $f$ it holds that $f(\E[\rvx]) \leq \E[f(\rvx)]$.}
% \begin{equation}
\begin{align}
    \log p_{\theta}(\rvx) &=  \log \int p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz)d\rvz \label{eq:intro_elbo_def_1}\\
   &=  \log \int \frac{q(\rvz)}{q(\rvz)} p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz)d\rvz \label{eq:intro_elbo_def_2}\\
   &=  \log \E_{q(\rvz)} \frac{p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz)}{q(\rvz)} \label{eq:intro_elbo_def_3}\\
   &\geq  \E_{q(\rvz)}\log  \frac{p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz)}{q(\rvz)} \label{eq:intro_elbo_def_4}\\
   &\overset{def}{=} \mathcal{F}(\rvx, q, \theta). \notag
\end{align}
% \end{equation} 


This is a variational lower bound with \textit{variational} parameter $q$. It is also known as Evidence Lower BOund (ELBO), as it constitute a bound on the loglikelihood (evidence) of the model. 
We visualize ELBO  in Figure~\ref{fig:intro_bound}. Different variational distributions $q$ will correspond to different lower bounds (e.g. green and red dashed lines in Figure~\ref{fig:intro_bound}), while different values of parameters $\theta$, will correspond to different points on the curve. 
\begin{marginfigure}[1\baselineskip]
\begin{tikzpicture}
  % Plot settings
  \begin{axis}[
    xlabel=$\theta$, 
    % ylabel={Log likelihood}, 
    axis lines=left, 
    enlargelimits, 
    height=4.5cm, 
    width=5.4cm,
    % legend pos=north west + (0, -0.5cm),
    % legend pos=south,
    yticklabels={},
    ytick=\empty,   % Remove y-axis ticks
    xlabel style={at={(axis description cs:1.0,+0.2)},anchor=south},
    legend style={at={(0,.86)}, anchor=south west, draw=none, fill=none},
    % title={Variational Lower Bound Visualization}
  ]
    
    % True log likelihood (log p(x|theta))
      \addplot[domain=-3:3, samples=100, very thick, RoyalBlue] {ln(2*exp(-(x-1)^2) + 0.1*exp(-(x+1.75)^2))};
    \addlegendentry{$\log p_{\theta}(\rvx)$}

    % Variational lower bound option 1
    \addplot[domain=-1:3., samples=100, very thick, RedOrange, dashed] {ln( 100 * exp(-(x-1.5)^2/0.5) + exp(-(x)^2)) - 4.7};
    \addlegendentry{$\mathcal{F}(\rvx, q^{(1)}, \theta)$}
    % Variational lower bound option 2
    \addplot[domain=-3:1, samples=100, very thick, ForestGreen, dashed] {ln( 10 * exp(-(x+1.5)^2/0.75) + exp(-(x)^2)) - 5.};
    \addlegendentry{$\mathcal{F}(\rvx, q^{(2)}, \theta)$}

    % KL Divergence (gap between the curves)
    % \node at (axis cs:-1, -2) [anchor=north] {KL Divergence};
    % \draw[<->, thick] (axis cs:0, -0.7) -- (axis cs:0, -1.7);

  \end{axis}
\end{tikzpicture}
\caption{Illustration of the log likelihood (solid line) and two variational lower bounds (dashed lines) corresponding to different variational distributions $q$.}\label{fig:intro_bound}
\end{marginfigure}

To see how ELBO relates to maximum likelihood and posterior approximation,  consider the following decomposition:
\begin{equation}
\begin{aligned}
    \mathcal{F}(\rvx, q, \theta) &= \E_{q(\rvz)}\log  \frac{p_{\theta}(\rvx,\rvz)}{q(\rvz)} \\
    & = \E_{q(\rvz)}\log  \frac{p_{\theta}(\rvz|\rvx)p_{\theta}(\rvx)}{q(\rvz)} \\
    & = \E_{q(\rvz)}\log  \frac{p_{\theta}(\rvz|\rvx)}{q(\rvz)} +  \log p_{\theta}(\rvx)\\
    & = - \KL{q(\rvz)}{p_{\theta}(\rvz|\rvx)} + \log p_{\theta}(\rvx).\\
\end{aligned}
\end{equation}

This shows that ELBO is equal to the sum of marginal likelihood and the negative KL divergence between the introduced distribution $q(\rvz)$ and the true posterior.
% and makes it clear that the bound is tight when $q(\rvz) = p(\rvz|\rvx)$. 
As \citet{neal1998view} showed,  maximizing ELBO with respect to the distribution $q$ is identical to minimizing the KL divergence. When $q(\rvz) = p(\rvz|\rvx)$ the divergence is zero and the ELBO matches the log likelihood.
Moreover, \citet{neal1998view} showed that if $\mathcal{F}(\rvx, q, \theta) $ has a maximum at $q^*, \, \theta^*$, then $\log p_{\theta}(\rvx)$ has a maximum at $\theta^*$. That is, ELBO has the same optimum as the maximum likelihood objective. 

\paragraph{Expectation Maximization}
Expectation Maximization Algorithm, also known as a EM-algorithm~\citep{dempster1977maximum} is an iterative algorithm to infer posterior over the latent variables and the maximum likelihood estimate of the parameters.
It is usually applied to a class of problems, where $p_{\theta}(\rvz|\rvx)$ is tractable if we fix parameters of the generative model $\theta$. Each iteration consists of \textit{expectation step} (E-step) and \textit{maximization step} (M-step) defined below.
\begin{equation}
\begin{aligned}
    \text{\textbf{E-step:  }} &q^{t+1}_n(\rvz) = p_{\theta^t}(\rvz|\rvx_n) = \arg\max_{q_n} \mathcal{F}(\rvx_n, q_n, \theta^t),\;\;\forall n \in \{1\dots N\}\\
    \text{\textbf{M-step:  }} &\theta^{t+1} = \arg\max_{\theta} \E_{p_e(\rvx)}\mathcal{F}(\rvx, q^{t+1}_n, \theta)\\
\end{aligned}
\end{equation}

The expectation step computes the posterior distribution given the model parameters from the previous step. Maximization step uses this posterior to optimize the ELBO with respect to parameters $\theta$. From the formulation above, we also see that both steps of the EM-algorithm solve the same optimization problem: maximizing the ELBO.  This observation allows us to generalize this to a wider class of problems where the posterior distribution is not tractable. 

\paragraph{Variational Inference}
When EM-algorithm is intractable, one can still use ELBO as a training objective. 
In order to do that, assume that $q_n$,  variational posterior distribution for a datapoint $\rvx_n$ belongs to a parametric family of distributions defined by parameters $\phi_n$:
\begin{equation}
    \mathcal{Q} = \{q_{\phi_n}(\rvz) | \phi_n \in \Phi\}
\end{equation}

Note that we will assume that each of $q_{\phi_n}(\rvz)$ is independent of each other, which is known as a \textit{mean-field assumption} and we call $\phi_n$ local parameters, since they are learned separately for each datapoint, as opposed to global parameters $\theta$. The resulting Variational Inference (VI) objective is: 
\begin{equation}
     \mathcal{L}^{VI}(\rvx_n, \phi_n, \theta) =  \E_{q_{\phi_n}(\rvz)}\log  \frac{p_{\theta}(\rvx_n|\rvz)p_{\theta}(\rvz)}{q_{\phi_n}(\rvz)}.
\end{equation}
\begin{marginfigure}
\begin{tikzpicture}
    % Draw the circle representing the variational family
    \draw[very thick, ForestGreen, rotate=35] (0,0) ellipse (1.8cm and 0.87cm) node[above, yshift=0.3cm, xshift=-0.2cm]  {\large$\mathcal{Q}$};
    
    % Point for the true posterior outside the circle
    \node[RedOrange] (trueposterior) at (2.8, -1.1) {\Large$\bullet$};
    \node[below] at (trueposterior) {\normalsize$ p_{\theta}(z \mid x) $};
    
    % Point for the variational distribution on the border of the circle
    \node[RoyalBlue!50] (varfamily_init) at (-0.23, -.5) {\Large$\bullet$};
    \node[below left] at (varfamily_init) {\normalsize $q^{\text{init}}_{\phi_n}(z) $};
    
    \node[RoyalBlue] (varfamily) at (1.23, -.1) {\Large$\bullet$};
    \node[above left] at (varfamily) {\normalsize $q^*_{\phi_n}(z) $};

     % Curvy line between varfamily_init and varfamily
    \draw[thick, RoyalBlue!50, ->, dashed, bend left=20] (varfamily_init) to (varfamily);
    
    % Arrow from variational family to true posterior
    \draw[-, dashed, thick] (varfamily) -- (trueposterior) node[midway, above, sloped] {$\KL{q}{p}$};

    % Labeling the circle
    % \node at (0, -3.5) {Parameter space of \( \lambda \)};

\end{tikzpicture}
\caption{When posterior distribution $ p_{\theta}(z \mid x) $ lies outside of variational family $\mathcal{Q}$, variational inference seeks to find the approximation $q^*_{\phi_n}(z) $ which is closest to the true posterior in terms on KL divergence} \label{fig:intro_vi}
\end{marginfigure}
Optimizing this objective function yields $q_{\phi_n}$ from a chosen family of distributions, which is closest to the true posterior in terms of KL divergence (see Figure~\ref{fig:intro_vi} for illustration). Similarly to the stochastic version of the gradient descent, stochastic Variational Inference (SVI) was proposed~\citep{hoffman2013stochastic}, where at each iteration only a subset of data is used and thus only a subset of local variational parameters $\phi_n$ is updated. 
However, there are still multiple aspects that hinder the scalability of the VI objective. We will now address each of these in detail. 


\paragraph{Amortized Variational Inference}
Even with stochastic VI, having separate local parameters for each data point can be very expensive. 
A common way to overcome it is to use amortization. This approach restricts the family of variational distributions even further, assuming that all the $q_n$ are parametrized with the same Neural Network $f_{\phi}$ which maps datapoints $\rvx_n$ to the corresponding parameters of the distribution: 
\begin{equation}
    % \mathcal{Q}_{\text{A}} = \Big\{ q: \prod_n q_{\phi}(\rvz | \rvx_n) \Big\}
    q_{\phi_n}(\rvz) = q_{\phi}(\rvz |\rvx_n).
\end{equation}

The amortized family is a subset of the mean-field variational family and the difference in the quality of the resulting approximations is known as \textit{Amortization Gap}~\citep{cremer2018inference}. It was also shown that a middle ground between AVI and SVI achieves better performance~\citep{kim2018semi}. However, in a large scale, even a small computational overhead can be detrimental. Therefore, throughout this thesis, we will use amortized variational inference objective:
\begin{equation}\label{eq:intro_avi}
     \mathcal{L}^{AVI}(\rvx, \phi, \theta) =  \E_{q_{\phi}(\rvz)}\log  \frac{p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz)}{q_{\phi}(\rvz|\rvx)}.
\end{equation}

\paragraph{Reparametrization trick and doubly stochastic variational inference}
Finally, in order to learn both generative parameters $\theta$ and variational parameters $\phi$, we need to know how to compute the gradients. First, the parametric form of $p_{\theta}(\rvx|\rvz)$, $p_{\theta}(\rvz)$ and $q_{\phi}(\rvz|\rvx)$ allows us to compute the corresponding gradients $\nabla_{\theta}\log p_{\theta}(\rvx|\rvz)$, $\nabla_{\theta}\log p_{\theta}(\rvz)$ and $\nabla_{\theta}\log q_{\phi}(\rvz|\rvx)$ using backpropagation. Given that, the gradient of the Evidence Lower Bound w.r.t $\theta$ is straightforward:
\begin{equation}
    \begin{aligned}
      \nabla_{\theta}\mathcal{L}^{AVI}(\rvx, \phi, \theta) 
      &= \E_{q_{\phi}(\rvz|\rvx)} \nabla_{\theta} \log  \frac{p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz)}{q_{\phi}(\rvz)} \\
      &= \E_{q_{\phi}(\rvz|\rvx)} \nabla_{\theta} \log p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz).
    \end{aligned}
\end{equation}

This quantity can be efficiently computed using Monte Carlo estimation. In practice, even one-sample estimation works well:

\begin{equation}
\begin{aligned}
    \nabla_{\theta}\mathcal{L}^{AVI}(\rvx, \phi, \theta) &\approx \nabla_{\theta} \log p_{\theta}(\rvx|\tilde{\rvz})p_{\theta}(\tilde{\rvz}),\\
    \tilde{\rvz} &\sim q_{\phi}(\rvz|\rvx)
    \end{aligned}
\end{equation}

However, since in Eq.~\ref{eq:intro_avi} the expectation is taken with respect to $q_{\phi}(\rvz|\rvz)$, the gradient computation is less straightforward. It can be computed using the method called REINFORCE~\citep{williams1992simple} or with the help of the reparameterization trick. The former uses log-derivative trick to represent the gradient as an expectation. However, it tends to have a high variance and is not often used in generative modeling. The reparameterization trick, on the other hand, makes use of the change of variable formula and is very widespread. This method was independently proposed for latent variable generative models \cite{kingma2014autoencoding, rezende2014stochastic} and for Bayesian inference \citep{titsias2014doubly}. 

Consider a random variable $\varepsilon \sim p(\varepsilon)$ and a deterministic transformation $g_{\phi}(\varepsilon, \rvx)$, such that:
\begin{equation}
    \tilde{\rvz} = g_{\phi}(\varepsilon, \rvx)\quad \Rightarrow \quad  \tilde{\rvz} \sim q_{\phi}(\rvz|\rvz).
\end{equation}

We can now use this deterministic transformation to apply change of variable formula to the Eq.~\ref{eq:intro_avi}:
\begin{equation}
\begin{aligned}
     \mathcal{L}^{AVI}(\rvx, \phi, \theta) &=  \E_{p(\varepsilon)}\log  \frac{p_{\theta}(\rvx|\tilde{\rvz})p_{\theta}(\tilde{\rvz})}{q_{\phi}(\tilde{\rvz}|\rvx)},\\
     \text{where}\; \tilde{\rvz} &= g_{\phi}(\varepsilon, \rvx).
\end{aligned}
\end{equation}

In this way, we move all the trainable parameters inside the expectation and the gradient with respect to $\phi$ can be computed using the Monte Carlo estimate. 
\begin{equation}
\begin{aligned}
    \nabla_{\phi}\mathcal{L}^{AVI}(\rvx, \phi, \theta) &\approx  \nabla_{\phi}\log  \frac{p_{\theta}(\rvx|\tilde{\rvz})p_{\theta}(\tilde{\rvz})}{q_{\phi}(\tilde{\rvz}|\rvx)}, \\
    \text{where}\; \tilde{\rvz} &= g_{\phi}(\tilde{\varepsilon}, \rvx)\,\text{ and }\, \tilde{\varepsilon} \sim p(\varepsilon).
\end{aligned}
\end{equation}

Note that this approach now has two sources of stochasticity. At each step, we use a subsample of the data points (a mini-batch) $\{\rvx_i\}_{i \in \mathcal{M}}$ to estimate the expectation with respect to the empirical distribution $p_{e}(\rvx)$ and we sample from the variational posterior (by the mean of the Reparametrization trick) to estimate the expectation $\E_{q_{\phi}(\rvz|\rvx)}$. \citet{titsias2014doubly} referred to such method as \textit{Doubly Stochastic Variational Inference} (DDVI). 
However, this term is more common in Bayesian inference rather than in Deep Generative Modeling. Therefore, throughout this thesis, we will mostly refer to it as Amortized Variational Inference with Reparametrization trick.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Auxiliarly variables: latent and non-latent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\section{Latent Variable Models}\label{sec:into_latent_variable_models}
%- Latent variable generative models: from PPCA to Diffusion models
%- $$p(x) = p(x|z)p(z)$$
%- Lower dimentional manifold assumption
%- Oldies but goldies: PPCA -> Helmholtz machines / Boltzman machines 
%- Modern heros: Variational Autoencoders, Diffusion Models
%- Amortization and voila, we have variational posterior $$q(z|x)$$
This thesis contributes to the field of Variational Autoencoders (VAE) and Diffusion Models. These are deep generative models with latent variables and are trained with the Amortized Variational Inference. We will now discuss both of these models in more details.
% We consider the application of these models to the problem of modeling the distribution of images in different scenarios. 

\subsection{Variational Autoencoder}
The first generative model we will consider is Variational Autoencoder. 
% Next, there exist several design choces that influnce the 
% This generative model can be interpreted as an infinite mixture model \cite{mattei2018leveraging}. VAEs are trained with the amortized variational inference described in Section~\ref{sec:intro_vi}.
% Here \cite{mattei2018leveraging}, this interpretation allowed to obtain an upper bound on the likelihood. 

% \subsection{ELBO as a training objective}
Consider the Evidence Lower Bounds (Eq.~\ref{eq:intro_avi}) that is used to train VAEs. There are multiple ways to rewrite this objective which give rise to different intuitions regarding the model. We start from the following decomposition:
\begin{equation}\label{eq:intro_elbo_again}
\mathcal{L}(\theta, \phi) = \E_{p_e(\rvx)}\Big[ \underbrace{\E_{q_{\phi}(\rvz|\rvx)} \log p_{\theta}(\rvx| \rvz)}_{\text{Reconstruction}} - \underbrace{\KL{q_{\phi}(\rvz|\rvx)}{p_{\theta}(\rvz)}}_{\text{KL term}} \Big].% \rightarrow \max_{\phi, \theta}
\end{equation}

\paragraph{Reconstruction}
\marginnote[4\baselineskip]{Variational posterior $q_{\phi}(\rvz|\rvx)$ is commonly called \textit{encoder} and the conditional likelihood $p_{\theta}(\rvx|\rvz)$ \textit{decoder}. Thus, when both distributions are learned together, the resulting model can be seen as a probabilistic version of the Autoencoder model.} 
The first part of the ELBO in Eq.~\ref{eq:intro_elbo_again} ensures that the model is able to reconstruct the input $\rvx$. If we consider it's Monte Carlo estimator for a single datapoint, it is computed by first sampling $\tilde{\rvz}_n$ for a given datapoint from the variational posterior
and then computing the conditional log-likelihood using this sample:
% \begin{equation}
% \begin{aligned}
\begin{align}
    \tilde{\rvz}_n &\sim q_{\phi}(\cdot|\rvx_n), \label{eq:intro_encode}\\
    \text{Re}(\rvx_n) &= \log p_{\theta}(\rvx_n|\tilde{\rvz}_n).\label{eq:intro_decode}
\end{align}
%     \end{aligned}
% \end{equation}
In other words, we \textit{encode} data into the latent space in Eq.~\ref{eq:intro_encode} and then \textit{reconstruct} it (or get parameters of the conditional likelihood given the latent variable) to compute the reconstruction in Eq.~\ref{eq:intro_decode}.

\paragraph{KL term}
The second term in Eq.~\ref{eq:intro_elbo_again} can be seen as a regularizer. It ensures that the variational posterior is close to the distribution $p_{\theta}(\rvz)$, which is usually called \textit{prior}.
Thus, after training a model, one can sample latent code from the prior \textit{unconditionally} and then decode it using the conditional likelihood. 
\marginnote[-2\baselineskip]{
The name \textit{prior} often suggests that this distribution is given to us \textit{apriori}. However, we will often employ an approach in which the distribution $p_{\theta}(\rvz)$ has learnable parameters and is learned together with the conditional likelihood. This is analogous to the empirical Bayes \cite{wang2019comment} approach, where prior is estimated from the data. 
}

\paragraph{Constrained Optimization Framework}
One can also consider ELBO objective form the constrained optimization point of view \citep{higgins2017beta, rezende2018taming}. \citet{higgins2017beta} introduce $\beta$-VAE, where additional hyperparameters $\beta$ it used to re-weight the KL-term in Eq.~\ref{eq:intro_elbo_again}. \citet{rezende2018taming} introduce Generalized ELBO with Constrained Optimization, where they treat the VAE objective as the minimization of the KL-divergence subject to the reconstruction constrain. The Lagrangian for this problem becomes ELBO with the additional parameters $\lambda$ (the Lagrange multiplier). They propose using the moving average of the constraint to update the Lagrange multiplier.
\citet{klushyn2019learning} proposes a different rule to update $\lambda$, which ensures that model optimizes ELBO once the reconstruction constraint is met. 

\paragraph{Approximating Likelihood with Importance Sampling}

As we have already mentioned, estimating likelihood exactly is hard for the latent variable model, due to the intractability of the integral. However, we can still estimate it using important sampling technique. For each datapoint $\rvx$ we could use the variational posterior distribution as a proposal. We should then sample $K$ timed from the proposal and compute the weighted average:
\begin{equation}
\begin{aligned}
    \log p_{\theta}(\rvx) = \int p_{\theta}(\rvx|\rvz)p_{\theta}(\rvz)d\rvz \approx &\log \frac1K \sum_k\frac{p_{\theta}(\rvx|\tilde{z}_k)p_{\theta}(\tilde{z}_k)}{q_{\phi}(\tilde{z}_k|\rvx)},\\
    &\tilde{z}_k \sim q_{\phi}(\rvz|\rvx).
\end{aligned}
\end{equation}

This is known as an \textit{importance weighted} log likelihood estimate. Notice that for $K=1$ it is equal to the ELBO. Furthermore, as we increase $K$ the bound on the log likelihood becomes tighter and approaches $\log p_{\theta}(\rvx)$ as $K \rightarrow \infty$ \citep{burda2015importance}. In practice $K=500$ or $K=1000$ is commonly used to report the test log-likelihood results and compare performance of different models. \citet{burda2015importance} proposed to use this as a training objective, a model known as \textit{Importance Weighted Autoencoder} (IWAE). 

% \paragraph{Information Theoretic Point of View}
% \addpart{fixing the broken ELBO}

% \paragraph{VAE limitations}
% \begin{itemize}
%     \item Blurry samples \cite{Rybkin2020-je}
%     \item Posterior Collapse \\
%     \cite{kingma2016improved} proposed free bits to solve posterior collapse    
%     \item Holes in the latent space\\
%     A more flexible prior to cover the aggregated posterior better
% \end{itemize}
 

% Auxiliary Deep Generative Models \cite{maaloe2016auxiliary}

% In this section, we focus in more details on the deep Latent Variable model (Eq.~\ref{eq:intro_lvm_def}). We assume continuous random variable $\rvz$ corresponding to each datapoint. 

\subsection{Conditional Likelihood}
\paragraph{Modeling pixel values}
Images are usually considered as continuous objects, where each pixel is assigned with the corresponding intensity $\nu$. 
However, the observed value $\rvx$ is usually discretized (e.g. intensity rounded to the nearest 8-bit representation). 
Therefore, using continuous distribution might be sub-optimal.
One option in this case would be to use Categorical distribution with 256 possible values for each pixel (in case of the 8-bit representation). However, this will ignore the fact that the nearby pixel values are correlated. 
Instead, a common approach is to model the pixel intensity as a continuous random variable and then discretize its distribution to calculate the actual likelihood. 
% Generally, there are two ways to implement this discretization which we will consider now. 

We will assume that all pixel across spacial and channel dimensions are conditionally independent:
\begin{align}
    p_{\theta}(\rvx|\rvz) &= \prod_i p_{\theta}(\rvx_i|\rvz),\\
    p_{\theta}(\rvx_i|\rvz) &= \sum_{k} P_{\theta}(\rvx_i = k|\rvz).
\end{align}
Where $\rvx_i$ is a single pixel of an image in an 8-bit representation and $k$ is a particular pixel value. 
Assume that pixel intensity has a continuous distribution with the probability density function $f_{\nu}$ and cumulative distribution function (CDF) $F_{\nu}$. We can discretize it in two ways. 

The first option is to split the support of the intensity random variable into bins and compute probability of each bin via integration:
\begin{equation}\label{eq:intro_int_discr}
\begin{aligned}
    &P_{\theta}(\rvx_i = k|\rvz) = \int_{\delta_{-}(\rvx_i)}^{\delta_{+}(\rvx_i)} f_{\nu}(\nu|\rvz)d\nu,\\
    &\delta_{-}(\rvx_i) =
    \begin{cases}
        \infty,\quad &\text{if } \rvx_i = 0\\
        \rvx_i + \frac12,\quad&\text{otherwise}
    \end{cases};  \quad
    \delta_{+}(\rvx_i) =
    \begin{cases}
        -\infty,\quad &\text{if } \rvx_i = 255\\
        \rvx_i - \frac12,\quad&\text{otherwise}
    \end{cases}.
\end{aligned}
\end{equation}

Assuming that we know the CDF of the intensity, we ca compute the value of the integral:
\begin{equation}
    P_{\theta}(\rvx_i = k|\rvz) =\begin{cases}
    F_{\nu}(\rvx_i + \frac{1}{2}) - F_{\nu}(\rvx_i - \frac{1}{2}),\quad &\text{if } k \in (1, \dots, 254) \\
    F_{\nu}(\rvx_i + \frac{1}{2}) - 0,\quad &\text{if } k = 0 \\
    1 - F_{\nu}(\rvx_i - \frac{1}{2}),\quad &\text{if } k = 255 \\
    \end{cases}.
\end{equation}
It is convenient to use logistic distribution~\cite{kingma2016improved}, as it's CDF can be easily computed:
\begin{equation}
    F^{\text{logistic}}(x; \mu, \sigma) = \frac{1}{1 + \exp (- \frac{x - \mu}{\sigma})}.
\end{equation}
\begin{marginfigure}
\begin{tikzpicture}
  \begin{axis}[
  name=plot1,
    domain=0:12, 
    samples=100, 
    xlabel=\normalsize$\nu$, 
    ylabel=\normalsize$f_{\nu}$, 
    axis lines=middle, 
    enlargelimits, 
    % title={Intensity}, 
    % height=4cm, 
    width=5.5cm,
    yticklabels={},
    ytick=\empty,   % Remove y-axis ticks
    ylabel style={at={(axis description cs:+0.15,1.05)},anchor=north},
  ]
    \addplot[very thick, RoyalBlue] {1/(sqrt(2*pi*6.25)) *  exp(-0.5 * (x-3)^2/6.25};
  \end{axis}

  \begin{axis}[
  name=plot2,
  at={(0,-4cm)},
    % domain=0:15, 
    % samples=100, 
    xlabel=\normalsize $ k $, 
    ylabel={\normalsize$ P(\rvx=k)$}, 
    axis lines=middle, 
    enlargelimits, 
    % title={Pixel Values}, 
    % height=4cm, 
    width=5.5cm,
    ybar,
    bar width=0.2,
    yticklabels={},
    ytick=\empty,   % Remove y-axis ticks
    ylabel style={at={(axis description cs:+0.25,1.05)},anchor=north},
  ]
   % Discrete points from the Gaussian distribution
    \addplot[RedOrange!60, fill=RedOrange!60] coordinates {
(0.2, 0.1588)
(1.2, 0.1155)
(2.2, 0.1464)
(3.2, 0.1585)
(4.2, 0.1464)
(5.2, 0.1155)
(6.2, 0.0779)
(7.2, 0.0450)
(8.2, 0.0222)
(9.2, 0.0093)
(10.2, 0.0033)
(11.2, 0.0009)
(12.2, 0.0002)
    };
     \addlegendentry{Eq.~\ref{eq:intro_int_discr}}
    % Discrete points from the Gaussian distribution
    \addplot[ForestGreen!60, fill=ForestGreen!60] coordinates {
(0.15, 0.0844)
(1.15, 0.1259)
(2.15, 0.1600)
(3.15, 0.1733)
(4.15, 0.1600)
(5.15, 0.1259)
(6.15, 0.0844)
(7.15, 0.0482)
(8.15, 0.0235)
(9.15, 0.0097)
(10.15, 0.0034)
(11.15, 0.0010)
(12.15, 0.0003)
    };
    \addlegendentry{Eq.~\ref{eq:intro_softmax_discr}}
    % \addplot[blue, thick] {1/(sqrt(2*pi*0.5)) *  exp(-0.5 * (x-5)^2/0.5};
  \end{axis}
    % Arrow from the first plot to the second plot
  % \draw[->, thick](plot1.south east) -- (plot2.north west) node[midway, right] {};

\end{tikzpicture}
\caption{Probability Density Function for the pixel intensity (top) and discretized probabilities (bottom)}
\end{marginfigure}
\citet{salimans2016improved} proposed to use a mixture of discretized logistic distributions to improve the flexibility the distribution. It was later used in deep hierarchical VAEs \citep{vahdat2020nvae, Child2020-ze}.

The second option is to re-normalize the probability of each bin:
\begin{equation}\label{eq:intro_softmax_discr}
\begin{aligned}
    P_{\theta}(\rvx_i = k|\rvz) = \frac{f_{\nu}(k|\rvz)}{\sum_{i=0}^K f_{\nu}(i|\rvz)}.
\end{aligned}
\end{equation}

Note that in this case, we only need to know the PDF of the pixel intensity up to a normalization constant. For example, we can assume Gaussian distribution of the pixel intensity and re-normalize using softmax:

\begin{equation}
\begin{aligned}
    P_{\theta}(\rvx_i = k|\rvz) = \frac{\exp\left(-\frac12 \left(\frac{\mu_{\theta}(\rvz) - k}{\sigma_{\theta}(\rvz)}\right)^2\right)}{\sum_{i=0}^{255} \exp\left(-\frac12 \left(\frac{\mu_{\theta}(\rvz) - i}{\sigma_{\theta}(\rvz)}\right)^2\right)}.
\end{aligned}
\end{equation}

There is one important difference between two approaches. The first discretization option assigns higher probability to the pixels 0 and 255. \citet{salimans2016improved} motivated this by the empirical observation that these values have higher frequency in real data. However, this has an effect that in order to learn confident prediction of the pixel values that are close to 0 or 255, the model has to learn a smaller variance of the intensity. Re-normalization, on the other hand, divide all bins by the same normalization constant.

% So the experiments
% Make the variance separatele learned parameter, initilize at zero. 

% Likelihood Options:
    % Gaussian tail CDF
    % Logistic tail CDF
    % Gaussian softmax

% Variance Otions:
    % No Limit
    % Clip 1/sigma at 1000
    % Clip 1/sigma at 500




\subsection{Learnable Prior}
\marginnote[2\baselineskip]{Note that this way prior does not have any trainable parameters, therefore, $\theta$ subscript is removed.}
Early VAE works used isotropic Gaussian with zero mean as a prior over the latent variables.
\begin{equation}
    p(\rvz) = \mathcal{N}(\rvz | 0, \text{I}).
\end{equation}

However, this can be too restrictive for a model to have and might cause a problem known in the VAE literature as \textit{holes} in the latent space. Intuitively, we can think of it as regions in the latent space with high prior probability but that are not covered by any of the variational posterior distributions. 

To understand the problem of holes in the latent space, consider a toy example with the 2-dimensional latent space in Figure~\ref{fig:intro_prior_mismatch}. The top plot depicts standard Gaussian prior and darkens of the color represent the probability of the sample. Let us also define the \textit{aggregated posterior} as a variational posterior averaged over the entire dataset: $q_{\phi}(\rvz) = \E_{p_{e}(\rvx)}q_{\phi}(\rvz|\rvx)$. For example, the bottom part of the Figure~\ref{fig:intro_prior_mismatch} presents a mixture of several Gaussian distributions that spread around the same area of the 2-d plane as the prior above it. However, there are several places where the aggregated posterior has a very low probability, namely, no real data point $\rvx$ is ever encoded in this part of the latent space. At the same time, the prior distribution assigns a high probability to this region. One such region is denoted by a red circle in the figure. 
This motivated the use of more complicated prior distributions, which are capable of \textit{adapting} to the aggregated posterior and are flexible enough to avoid the holes. 

\begin{marginfigure}
\vspace*{-4\baselineskip}
\subfloat[Prior \label{subfig:intro_prior}]{
\begin{tikzpicture}
\begin{axis}[
    width=5.5cm,
    height=5.5cm,
    % title={\normalsize Prior},
    axis lines=none,
    colormap={greenmap}{color(0cm)=(ForestGreen!1); color(1cm)=(ForestGreen!99)},
    point meta min=0, point meta max=1,
    view={0}{90}
]
\addplot3[
    surf,
    shader=interp,
    samples=30,
    domain=-2:2,
    domain y=-2:2
]{exp(-0.5*(x^2 + y^2) / 0.9^2) / 0.9};
\draw[very thick, RedOrange] (-0.7,0.2) circle (0.3cm); 
% node[above, yshift=0.3cm, xshift=-0.2cm]  {};
\end{axis}
\end{tikzpicture}
}
\vskip 5pt
\subfloat[Aggregated Posterior \label{subfig:intro_agg_posterior}]{%\subfigure{
\begin{tikzpicture}
\begin{axis}[
    % at={(0cm,-4.8cm)},
    width=5.5cm,
    height=5.5cm,
    % title={\normalsize Aggregated Posterior},
    axis lines=none,
    colormap={greenmap}{color(0cm)=(ForestGreen!1); color(1cm)=(ForestGreen!99)},
    point meta min=0, point meta max=1,
    view={0}{90}
]
\addplot3[
    surf,
    shader=interp,
    samples=30,
    domain=-1.5:1.5,
    domain y=-1.5:1.5
]
{
(1/6 * exp(-0.5*((x-1)^2 + y^2) / 0.35^2) / 0.35 
+ 1/6 * exp(-0.5*(x^2 + (y-1)^2) / 0.3^2) / 0.3 
+ 1/6 * exp(-0.5*((x-0.9)^2 + (y-1.1)^2) / 0.28^2) / 0.28 
+ 1/6 * exp(-0.5*((x)^2 + y^2) / 0.4^2) / 0.4 
+ 1/6 * exp(-0.5*((x-0.5)^2 + (y+0.8)^2) / 0.21^2) / 0.15
+ 1/6 * exp(-0.5*((x+0.75)^2 + (y-0.8)^2) / 0.21^2) / 0.25
+ 1/6 * exp(-0.5*((x+0.5)^2 + (y+0.5)^2) / 0.25^2) / 0.25 ) * (0.8 + 0.4*sin(100*x)*cos(100*y) + 0.5*cos(90*x*y))
};
\draw[very thick, RedOrange] (-0.7,0.2) circle (0.3cm); 
\end{axis}
\end{tikzpicture}
}
\caption{Example of the mismatch between the prior (top plot) and aggregated posterior (bottom)}\label{fig:intro_prior_mismatch}
\end{marginfigure}

A prior flexibility can be achieved if we use a deep generative model for $p_{\theta}(\rvz)$. 
Note that we need to choose models with fully tractable densities to ensure the tractability of the ELBO objective.
% However, we need to have access the density to keel the ELBO objective tractable. 
\citet{chen2016variational} proposed to employ Autoregressive Normalizing Flows as a learnable prior in the VAEs. As we discussed in Section~\ref{sec:gen_zoo}, this class of models employs a change-of-variable formula to explicitly compute the model density. 

Even with a deep generative model as prior, we might still ask ourselves, what is the optimal prior that would maximize the Evidence Lower bound. 
To answer this question, we will follow \citet{hoffman2016elbo} and decompose the KL-term of the ELBO:
\marginnote[2\baselineskip]{We first apply definition of the KL-divergence and multiply the integrand by $1$. Then using the property of the logarithm, we split equation into two terms. }
\begin{align}
&\KL{q_{\phi}(\rvz|\rvx)}{p_{\theta}(\rvz)} = \int q_{\phi}(\rvz|\rvx) \log \frac{q_{\phi}(\rvz|\rvx)}{p_{\theta}(\rvz)}\frac{q_{\phi}(\rvz)}{q_{\phi}(\rvz)}d\rvz\\
 &\quad=\int q_{\phi}(\rvz|\rvx) \log \frac{q_{\phi}(\rvz)}{p_{\theta}(\rvz)}d\rvz  + \int  q_{\phi}(\rvz|\rvx) \log \frac{q_{\phi}(\rvz|\rvx)}{q_{\phi}(\rvz)}d\rvz  \\
 &\quad=\int q_{\phi}(\rvz|\rvx) \log \frac{q_{\phi}(\rvz)}{p_{\theta}(\rvz)}d\rvz  + \int  q_{\phi}(\rvz|\rvx) \log \frac{q_{\phi}(\rvz, \rvx)}{q_{\phi}(\rvz) p_e(\rvx)}d\rvz. 
\end{align}
Now, we can take an expectation with respect to the data distribution.
% to  get the sum up we get the following decomposition of the KL-term:

\marginnote[2\baselineskip]{We notice that both terms can be defined as a KL divergences. Furthermore, the second term is equal to the Mutual Information between observation and latent code, since it is defined as a KL divergence between the joint distribution and a product of marginal distributions: $\text{MI}\left[\rvx, \rvy \right] = \KL{p(\rvx, \rvy)}{p(\rvx)p(\rvy)}$.}
\begin{align}\label{eq:intro_kl_to_vamp}
\E_{p_e(\rvx)} \KL{q_{\phi}(\rvz|\rvx)}{p_{\theta}(\rvz)} = &\int \underbrace{\E_{p_e(\rvx)} q_{\phi}(\rvz|\rvx)}_{q_{\phi}(\rvz)} \log \frac{q_{\phi}(\rvz)}{p_{\theta}(\rvz)}d\rvz  \\
&\quad\quad+ \iint  \underbrace{q_{\phi}(\rvz|\rvx)p_e(\rvx)}_{q_{\phi}(\rvz, \rvx)} \log \frac{q_{\phi}(\rvz, \rvx)}{q_{\phi}(\rvz) p_e(\rvx)}d\rvz d\rvx \notag\\
= &\KL{q_{\phi}(\rvz)}{p_{\theta}(\rvz)}
+ \text{MI}\left[\rvz; \rvx\right].
\end{align}

% \marginnote{It is very common to use Gaussian distribution for $q_{\phi}(\rvz|\rvx)$. In this case, aggregated posterior is a Gaussian mixture.}
Note that the prior distribution is only present in the first term: KL-divergence between the aggregated posterior and prior. Therefore, the prior that minimizes the ELBO is equal to the aggregated posterior, the \textit{mixture} of encoders:
\begin{equation}
    p^*(\rvz) = \arg\max_{p(\rvz)}\mathcal{L}(\theta, \phi) = \E_{p_e(\rvx)}q_{\phi}(\rvz|\rvx).
\end{equation}
Therefore, this decomposition highlights the importance of multimodal priors. 
Motivated by this observation, \citet{tomczak2018vae} proposed a VampPrior, a trainable prior for VAEs, which is defined as a mixture of variational posteriors. In order to avoid overfitting and make the model computationally tractable, this mixture is parameterized by $K$ pseudoinputs instead of the whole training dataset:
\begin{equation}
    p_{\theta}(\rvz) = \frac1K \sum_k q_{\phi}(\rvz|u_k) \approx p_{\theta}^*(\rvz).
\end{equation}
In this case, the pseudoinputs themselves are the trainable part of the prior. In this thesis, we explore how VampPrior can be used to improve density estimation in a dynamic \textit{continual learning} setting. 

 % Not only does the VampPrior introduces a more flexible prior, but also it is implicitly forces to "memorize" training data. 

\begin{marginfigure}
\begin{tikzpicture}[>=Stealth, every node/.style={scale=1.0}, line width=1.2pt]
    % Task 1 x=0, label = 1    
    \foreach \y in {-1, 0, 1} {
        \node[circle, draw, fill=white, minimum size=0.3cm] (input0\y) at (-0.25, 0.5*\y+0.5) {};
        \node[circle, draw, fill=ForestGreen!50, minimum size=0.3cm] (output0\y) at (.25, 0.4*\y+0.5) {};
    }
    \foreach \y in {-1, 0, 1} {
        \foreach \z in {-1, 0, 1} {
            \draw[thick] (input0\y) -- (output0\z);
        }
    }
    \node[] (train0) at (0, -.9)  {\normalsize\color{ForestGreen}\(\mathcal{D}^{(1)}_{\text{train}}\)};
    
    % Task 1 x=1.5, label = 2    
    \foreach \y in {-1, 0} {
        \node[circle, draw, fill=white, minimum size=0.3cm] (input1\y) at (1.25, 0.5*\y+0.5) {};
        \node[circle, draw, fill=RoyalBlue!50, minimum size=0.3cm] (output1\y) at (1.75, 0.4*\y+0.5) {};
    }
    \node[circle, draw, fill=white, minimum size=0.3cm] (input11) at (1.25, 0.5*1+0.5) {};
    \node[circle, draw, fill=ForestGreen!50, minimum size=0.3cm] (output11) at (1.75, 0.4*1+0.5) {};
    
    \foreach \y in {-1, 0, 1} {
        \foreach \z in {-1, 0, 1} {
            \draw[thick] (input1\y) -- (output1\z);
        }
    }
    \node[] (train1) at (1.5, -.9)  {\normalsize \color{RoyalBlue}\(\mathcal{D}^{(2)}_{\text{train}}\)};

    % Task 1 x=3, label = 2    
    \node[circle, draw, fill=white, minimum size=0.3cm] (input2-1) at (2.75, -0.5*1+0.5) {};
    \node[circle, draw, fill=RedOrange!50, minimum size=0.3cm] (output2-1) at (3.25, -0.4*1+0.5) {};
    \node[circle, draw, fill=white, minimum size=0.3cm] (input20) at (2.75, 0.5) {};
    \node[circle, draw, fill=RoyalBlue!50, minimum size=0.3cm] (output20) at (3.25, 0.5) {};

    \node[circle, draw, fill=white, minimum size=0.3cm] (input21) at (2.75, 0.5*1+0.5) {};
    \node[circle, draw, fill=ForestGreen!50, minimum size=0.3cm] (output21) at (3.25, 0.4*1+0.5) {};
    \foreach \y in {-1, 0, 1} {
        \foreach \z in {-1, 0, 1} {
            \draw[thick] (input2\y) -- (output2\z);
        }
    }
    \node[] (train2) at (3, -.9)  {\normalsize \color{RedOrange}\(\mathcal{D}^{(3)}_{\text{train}}\)};
    
    % Timeline
    \draw[->, very thick] (-0.5, -1.5) -- (4, -1.5) node[above right] {\(t\)};
    \foreach \x/\label in {0/1, 1.5/2, 3/3} {
        \draw[very thick] (\x, -1.7) -- (\x, -1.3) node[below=0.3cm] {\(\label\)};
    }
\end{tikzpicture}
\caption{Illustration of the continual learning, where a model is trained on three different tasks sequentially and is expected to generalize on the data from all the tasks. }\label{fig:intro_cl_plot}
\end{marginfigure}

% \addpart{Explain More on Continual Learning}
Continual learning is a challenging problem of training deep neural networks in an adaptive manner. We assume a sequence of tasks $t=1,\dots,T$ that have to be solved by a single model.
Subsets of the data for each task $\train^1, \dots ,\train^T$ arrive sequentially, and we have access to only one chunk of data at a time (e.g. $\train^t$ at time $t$). In Figure \ref{fig:intro_cl_plot}, we show this setup schematically for the 3 tasks. At each timestamp, we train our model on a new chunk of data (\textcolor{ForestGreen}{Task1}, \textcolor{RoyalBlue}{Task2} and \textcolor{OrangeRed}{Task3}). In the end, we expect it to generalize well on the data from all three tasks. Or, in other words, we expect to obtain a generative model that is approximating the data distribution that covers data from all three tasks.   

DNNs are known to reduce their quality on previously learned tasks when trained on data from a new one \citep{kirkpatrick2017overcoming}. This phenomenon was called \textit{catastrophic forgetting} \citep{mccloskey1989catastrophic} in the literature. Since both inference $\Enc{\rvz}{\rvx}$ and generative $\Dec{\rvx}{\rvz}$ models are parameterized by neural networks, the problem is also relevant for VAEs.  

In addition to being a flexible prior distribution, VampPrior also learns a compact representation of the training dataset. If the model is well trained, VampPrior approximates the full dataset, using $K$ pseudoinputs. 
Furthermore, it is straightforward to expand the expressivity of the mixture distribution by adding a new component.
This motivates the use of this mixture prior in continual learning and raises the first research question in this thesis:

\begin{quote}
        \marginnote[-0\baselineskip]{\normalsize Research Question 1}
	\textit{How to use Variational Mixture of Posteriors Prior to improve density estimation in continual learning?}
\end{quote}
This research question is addressed in Chapter~\ref{chap:boovae}, where we derive an optimal prior objective for the continual learning setting and propose a greedy algorithm to add components to the mixture when new data arrive. 


\subsection{Hierarchical VAEs}
In the previous section, we have discussed how to improve the VAE model using more flexible and trainable priors.
A similar idea can be developed if we present the distribution in the latent space using a product rule of probability, similarly to the autoregressive models discussed in Section~\ref{sec:into_deep_generative_models}. Let us split the latent space into $L$ groups and assume the the prior distribution has the following autoregressive structure:
\begin{equation}
    p_{\theta}(\rvz) = \prod_{l=1}^L p_{\theta}(\rvz_l|\rvz_{>l}),
\end{equation}
where $\rvz_{<l} = (\rvz_1, \dots, \rvz_{l-1})$ and assume $\rvz_{<1} = \varnothing$. We will call each group of latent variables $\rvz_l$ a \textit{stochastic layer}. This decomposition imposes a structure on the latent space, which is often referred to as hierarchy, and the corresponding generative model is called \textit{Hierarchical VAE}. 

With the hierarchical structure in the prior, it is reasonable to assume the variational posterior to have an autoregressive structure as well. Depending on the order in which variational posterior is decomposed, we will distinguish BottomUp and TopDown Hierarchical VAEs. 
% In Figure~\ref{fig:intro_hierarchical_schema} we 

\paragraph{BottomUp Inference}
We start with the Bottom Up model, where the variational posterior is defined in the following way:
\begin{equation}
    q_{\phi}(\rvz|\rvx) = \prod_{l=1}^L q_{\phi}(\rvz_l|\rvz_{<l}, \rvx).
\end{equation}
\begin{marginfigure}
% \subfloat[Bottom Up VAE
 % \label{subfig:introbottom_up_vae}]{
     \begin{tikzpicture}[node distance=.4cm]
    \node[obs, minimum size=0.75cm] (X) {$\rvx$};
  \node[latent, minimum size=0.75cm, above=0.2cm of X] (z1) {$\rvz_1$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z1] (z2) {$\rvz_2$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z2] (z3) {$\rvz_3$};
  \node[inner sep=0,minimum size=0.5cm, above=0.05cm of z3] (k_top) {$q_{\phi}(\rvz|\rvx)$};
  
  \node[inner sep=0,minimum size=0.5cm, left=0.05cm of z1] (k1) {}; % invisible node
  \node[inner sep=0,minimum size=0.5cm, right=0.1cm of X] (k2) {}; % invisible node
  \node[inner sep=0,minimum size=0.5cm, left=0.05cm of z3] (k3) {}; % invisible node
  % ENCODER:ALL ARROWS 
  \draw[-Latex, black] (X) -- (k2.center) |- node[above]{} (z3);
  \draw[-Latex, black] (X) -- (k2.center) |- node[above]{} (z2);
  \draw[-Latex, black] (X) -- (k2.center) |- node[above]{} (z1);
  \draw[-Latex, black] (z1) -- (k1.center) |- node[above]{} (z3);
  \edge[-Latex, black] {z2} {z3};
  \edge[-Latex, black] {z1} {z2};
  \end{tikzpicture}
  % \quad
  \begin{tikzpicture}[node distance=.4cm]
   \node[obs, minimum size=0.75cm] (X_gen) {$\rvx$};
  \node[latent, minimum size=0.75cm, above=0.2cm of X_gen] (z1_gen) {$\rvz_1$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z1_gen] (z2_gen) {$\rvz_2$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z2_gen] (z3_gen) {$\rvz_3$};
  \node[inner sep=0,minimum size=0.5cm, above=0.05cm of z3_gen] (k333) {$p_{\theta}(\rvx, \rvz)$};
  
    \node[inner sep=0,minimum size=0.5cm, left=0.05cm of z3_gen] (k4) {}; % invisible node

    \node[inner sep=0,minimum size=0.5cm, right=0.05cm of z3_gen] (k5) {}; % invisible node
    \node[inner sep=0,minimum size=0.5cm, right=0.05cm of z2_gen] (k6) {}; % invisible node
    \node[inner sep=0,minimum size=0.5cm, right=0.05cm of z1_gen] (k7) {}; % invisible node
    
    % \node[inner sep=0,minimum size=0.5cm, left=0.2cm of zL_gen] (k8) {}; % invisible node
    
    % shared path
    \edge[-Latex, black] {z3_gen} {z2_gen};
    \edge[-Latex, black] {z2_gen} {z1_gen};
    \draw[-Latex, black] (z3_gen) -- (k4.center) |- node[above]{} (z1_gen);

    % conditional likelihood
    \draw[-Latex, black] (z3_gen) -- (k5.center) |- (X_gen);
    \draw[-Latex, black] (z2_gen) -- (k6.center) |- (X_gen);
    \draw[-Latex, black] (z1_gen) -- (k7.center) |- (X_gen);
\end{tikzpicture}
% }
\caption{A graphical model for hierarchical VAEs with three stochastic levels and  \textbf{BottomUp} structure.}\label{subfig:introbottom_up_vae}
\end{marginfigure}
Let us consider the ELBO for this model:
\begin{align}
    \mathcal{L}(\rvx, \theta, \phi) &= \E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log p_{\theta}(\rvx|\rvz_{1:L}) + \E_{q_{\phi}(\rvz_{1:L}|\rvx)} \log \frac{\prod_{l=1}^L q_{\phi}(\rvz_l|\rvz_{<l}, \rvx)}{\prod_{l=1}^L p_{\theta}(\rvz_l|\rvz_{>l})} \label{eq:intro_bottom_up_elbo_1}\\
    &=\E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log p_{\theta}(\rvx|\rvz_{1:L}) +  \sum_{l=1}^L \E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log \frac{ q_{\phi}(\rvz_l|\rvz_{<l}, \rvx)}{p_{\theta}(\rvz_l|\rvz_{>l})}.\label{eq:intro_bottom_up_elbo_2}
\end{align}

Note that even though the KL term is divided into the sum of $L$ terms, each of them has an expectation over all the stochastic layers $\rvz_1\dots\rvz_L$. That being said, we would still need to sample all the latent variables to get a Monte Carlo estimation of each of the terms. 

We will return to this type of model in Section~\ref{sec:intro_diffusion}, where we will talk about latent variable perspective on diffusion models. 



\paragraph{TopDown Inference}
A different decomposition, which allows a more efficient implementation, was proposed by \citet{sonderby2016ladder}.  The idea is to choose the same traversal order as in the prior:

\begin{equation}
    q_{\phi}(\rvz|\rvx) = \prod_{l=1}^L q_{\phi}(\rvz_l|\rvz_{>l}, \rvx).
\end{equation}
The first advantage of this decomposition is the possibility for parameter sharing. For example, if we consider example in Figure~\ref{fig:intro_top_down}, in both prior and variational posterior $\rvz_2$ depends on $\rvz_3$, but not on $\rvz_1$, this we can share the parameters of the Neural Network that takes as input $\rvz_3$ and extract the features relevant to model the distribution of the $\rvz_2$. This shared paths are depicted with \textcolor{RoyalBlue}{blue arrows}.

\begin{marginfigure}
    \begin{tikzpicture}[node distance=.4cm]
  % ENCODER: ALL RV
  \node[obs, minimum size=0.75cm] (X) {$\rvx$};
  \node[latent, minimum size=0.75cm, above=0.2cm of X] (z1) {$\rvz_1$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z1] (z2) {$\rvz_2$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z2] (z3) {$\rvz_3$};
  \node[inner sep=0,minimum size=0.5cm, above=0.05cm of z3] (k_top) {$q_{\phi}(\rvz|\rvx)$};
  
  \node[inner sep=0,minimum size=0.5cm, right=0.1cm of X] (k2) {}; % invisible node
  \node[inner sep=0,minimum size=0.5cm, left=0.05cm of z3] (k3) {}; % invisible node
  % ENCODER:ALL ARROWS 
  \draw[-Latex, black] (X) -- (k2.center) |- node[above]{} (z3);
  \draw[-Latex, black] (X) -- (k2.center) |- node[above]{} (z2);
  \draw[-Latex, black] (X) -- (k2.center) |- node[above]{} (z1);
  \draw[-Latex, RoyalBlue, thick] (z3) -- (k3.center) |- node[above]{} (z1);
  \edge[-Latex, RoyalBlue, thick] {z3} {z2};
  \edge[-Latex, RoyalBlue, thick] {z2} {z1};
  \end{tikzpicture}
  \begin{tikzpicture}[node distance=.4cm]
   \node[obs, minimum size=0.75cm] (X_gen) {$\rvx$};
  \node[latent, minimum size=0.75cm, above=0.2cm of X_gen] (z1_gen) {$\rvz_1$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z1_gen] (z2_gen) {$\rvz_2$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z2_gen] (z3_gen) {$\rvz_3$};
  \node[inner sep=0,minimum size=0.5cm, above=0.05cm of z3_gen] (k333) {$p_{\theta}(\rvx, \rvz)$};
  
    \node[inner sep=0,minimum size=0.5cm, left=0.05cm of z3_gen] (k4) {}; % invisible node
    \node[inner sep=0,minimum size=0.5cm, right=0.05cm of z3_gen] (k5) {}; % invisible node
    \node[inner sep=0,minimum size=0.5cm, right=0.05cm of z2_gen] (k6) {}; % invisible node
    \node[inner sep=0,minimum size=0.5cm, right=0.05cm of z1_gen] (k7) {}; % invisible node
        
    % shared path
    \edge[-Latex, RoyalBlue, thick] {z3_gen} {z2_gen};
    \edge[-Latex, RoyalBlue, thick] {z2_gen} {z1_gen};
    \draw[-Latex, RoyalBlue, thick] (z3_gen) -- (k4.center) |- node[above]{} (z1_gen);

    % conditional likelihood
    \draw[-Latex, black] (z3_gen) -- (k5.center) |- (X_gen);
    \draw[-Latex, black] (z2_gen) -- (k6.center) |- (X_gen);
    \draw[-Latex, black] (z1_gen) -- (k7.center) |- (X_gen);
\end{tikzpicture}
\caption{A graphical model for hierarchical VAEs with three stochastic levels and  \textbf{TopDown} structure, which allows for parameter sharing (blue arrows).} \label{fig:intro_top_down}
\end{marginfigure}

Let us now consider the ELBO for this model:
\begin{align}
    \mathcal{L}(\rvx, \theta, \phi) &= \E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log p_{\theta}(\rvx|\rvz_{1:L}) + \E_{q_{\phi}(\rvz_{1:L}|\rvx)} \log \frac{\prod_{l=1}^L q_{\phi}(\rvz_l|\rvz_{>l}, \rvx)}{\prod_{l=1}^L p_{\theta}(\rvz_l|\rvz_{>l})} \\
    &=\E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log p_{\theta}(\rvx|\rvz_{1:L}) +  \sum_{l=1}^L \E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log \frac{ q_{\phi}(\rvz_l|\rvz_{>l}, \rvx)}{p_{\theta}(\rvz_l|\rvz_{>l})} \\
    &=\E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log p_{\theta}(\rvx|\rvz_{1:L}) +  \sum_{l=1}^L \E_{q_{\phi}(\rvz_{l},\rvz_{>l}| \rvx)}\log \frac{ q_{\phi}(\rvz_l|\rvz_{>l}, \rvx)}{p_{\theta}(\rvz_l|\rvz_{>l})} \\
    &=\E_{q_{\phi}(\rvz_{1:L}|\rvx)}\log p_{\theta}(\rvx|\rvz_{1:L}) \notag\\
    &\quad\quad\quad\quad\quad +  \sum_{l=1}^L \E_{q_{\phi}(\rvz_{>l}| \rvx)}\KL{q_{\phi}(\rvz_l|\rvz_{>l}, \rvx)}{p_{\theta}(\rvz_l|\rvz_{>l})}.
\end{align}

Here we get a KL term for each stochastic layers, however, the higher level term do not depend on the lower once at all. This might contribute to a lower variance of the Monte Carlo estimates.

There is still plenty of room to choose different types of parametrization in this type of probabilistic models. 
\citet{sonderby2016ladder} choose to have Gaussian distributions with Neural Network parametrization. 
\citet{kingma2016improved} proposed using the normalization flow as each stochastic layer. 
More large-scale experiments conducted by \citet{vahdat2020nvae} and \citet{Child2020-ze}, demonstrated the scalability of deep hierarchical VAEs and their ability to model density of high-dimensional and complex objects.
However, they can still exhibit training instabilities and tend to have a problem of posterior collapse, where some parts of the latent space do not carry information about the data. 

Another advantage of the TopDown structure is that it allows one to straightforwardly obtain the decomposition similar to Eq.~\ref{eq:intro_kl_to_vamp} for deep hierarchical VAEs and thus formulate the optimal prior for each stochastic layer as a corresponding mixture of variational posteriors. 
However, directly applying the VampPrior approximation with $K$ pseudoinputs becomes a very expensive approach, especially for the high-dimensional data on which these models are trained. This brings us to the next research question.
\begin{quote}
        \marginnote[0\baselineskip]{\normalsize Research Question 2}
	\textit{How to approximate VampPrior for hierarchical VAEs in a scalable way and improve latent space utilization?}
\end{quote}
This research question is addressed in Chapter~\ref{chap:dvp}, where we use the idea of amortization from variational inference literature to obtain a scalable approximation of the VampPrior. 

\subsection{Diffusion Models}\label{sec:intro_diffusion}
\paragraph{Latent Variable View on Diffusion Models}
Up to this point, we considered latent variable models, where we had to infer the posterior distribution from the data. This model has a lot of moving parts, namely, we are learning both generative model $p_{\theta}(\rvx, \rvz)$ and variational posterior $q_{\phi}(\rvz|\rvx)$ simultaneously.  And, as we mentioned earlier, training instabilities are common when scaling these models. 

In some cases, however, we are mostly interested only in having a good generative model and not interested in learning lower dimensional latent representation. In this case, we can use a much simpler family of variational posterior and focus on learning a powerful prior and conditional likelihood to fit the data.  

For example, diffusion-based generative model assume that the variational posterior for $L$ latent variable is defined by a diffusion process that starts are the data and terminates at the standard Gaussian distribution. That is, each latent variable is a noisy version of the input.  Then, generative model is essentially trained to "undo" the harm. Such generative models are called the denoising diffusion generation model. The connection between diffusion model and variational inference was studied extensively \cite{ho2020denoising, huang2021variational, kingma2021variational, tzen2019neural}.

Diffusion model can be seen as a BottomUp hierarchical VAE. However, additional assumptions about the probabilistic model and parameterization make it vary efficient and easy to scale. 
First, we introduce the Markov property to both generative model and variational posterior.
\begin{align}
    q_{\phi}(\rvz|\rvx) &= q_{\phi}(\rvz_1|\rvx)\prod_{l=2}^L q_{\phi}(\rvz_l|\rvz_{l-1}), \\
    p_{\theta}(\rvx, \rvz) &= p_{\theta}(\rvx |\rvz_1) p_{\theta}(\rvz_L)\prod_{l=1}^{L-1} p_{\theta}(\rvz_l|\rvz_{l+1}).
\end{align}

\begin{marginfigure}
\quad\quad
     \begin{tikzpicture}[node distance=.4cm]
    \node[obs, minimum size=0.75cm] (X) {$\rvx$};
  \node[latent, minimum size=0.75cm, above=0.4cm of X] (z1) {$\rvz_1$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z1] (z2) {$\rvz_2$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z2] (z3) {$\rvz_3$};
  \node[inner sep=0,minimum size=0.5cm, above=0.05cm of z3] (k_top) {$q_{\phi}(\rvz|\rvx)$};
  
  \edge[-Latex, black] {X} {z1};
  \edge[-Latex, black] {z1} {z2};
  \edge[-Latex, black] {z2} {z3};
  \end{tikzpicture}
  \quad\quad\quad
  \begin{tikzpicture}[node distance=.4cm]
   \node[obs, minimum size=0.75cm] (X_gen) {$\rvx$};
  \node[latent, minimum size=0.75cm, above=0.4cm of X_gen] (z1_gen) {$\rvz_1$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z1_gen] (z2_gen) {$\rvz_2$};
  \node[latent, minimum size=0.75cm, above=0.4cm of z2_gen] (z3_gen) {$\rvz_3$};
  \node[inner sep=0,minimum size=0.5cm, above=0.05cm of z3_gen] (k333) {$p_{\theta}(\rvx, \rvz)$};
  
    % arrows
    \edge[-Latex, black] {z3_gen} {z2_gen};
    \edge[-Latex, black] {z2_gen} {z1_gen};
    \edge[-Latex, black] {z1_gen} {X_gen};
\end{tikzpicture}
\caption{BottomUp hierarchical VAE with Markov property.}\label{fig:intro_diff_schema}
\end{marginfigure}

The graphical model of this Markov Hierarchical VAE is depicted in Figure~\ref{fig:intro_diff_schema}. Now each latent variable only depend on its nearest neighbor, but we still have a different traversal order. In order to be able to simplify the ELBO (Eq.~\ref{eq:intro_bottom_up_elbo_2}), we will make more assumptions about the variational posterior. Let us assume that each $q_{\phi}(\rvz_l|\rvz_{l-1})$ is a Gaussian distribution with \textit{linear} parametrization. 
% The ELBO for this model is the same as we derived in Eq.~\ref{eq:intro_bottom_up_elbo_2}
% However, this is still a Bottom Up hierarchical VAE and we will need to sample multiple latent variables for each term in the summation. 

% . The ELBO objective in Eq.~\ref{eq:intro_bottom_up_elbo} will be simpler, 
% Note that due to Markov assumption, each term in the ELBO objective depends on the local subset of latent variables. Namely, the Monte Carlo estimate of each term imply sampling $(\rvz_{l-1}, \rvz_{l}, \rvz_{l+1})$ from the variational posterior to then compute the log probability of $q(\rvz_l|\rvz_{l-1})$ and $p_{\theta}(\rvz_l|\rvz_{l+1})$ respectively. However, this can be simplified even further.


% Namely, assume that for each latent variable  $\rvz_l$ we will have a linear function $f_l$
% Namely, for a \textit{linear} function $f_l$, latent variable $\rvz_l$ will have the following variational posterior:
\begin{equation}
\begin{aligned}
    q(\rvz_l|\rvz_{l-1}) = \mathcal{N}(\rvz_l | \alpha_l \rvz_{l-1}, \sigma_l^2\text{I}).
\end{aligned}
\end{equation}

This assumption allows us to marginalize out any latent variables, for example to compute $q(\rvz_l|\rvx)$ in a closed form, which will also be a Gaussian with a linear mean function. Namely, each latent variable is a linear transformation of the input $\rvx$ with the added noise.
 We can use linearity assumption again to "reverse" the variational posterior, which will again be a Gaussian distribution:
 \marginnote[\baselineskip]{Note, that this is analogous to computing posterior in the linear Gaussian model with the likelihood $q(\rvz_l|\rvz_{l-1})$ and a prior $q(\rvz_{l-1}|\rvx)$ and can be computed in a closed form~\citep{bishop2006pattern}.}
\begin{align}
q(\rvz_{l-1} |\rvz_l, \rvx) &= \frac{q(\rvz_l|\rvz_{l-1}, \rvx)q(\rvz_{l-1}|\rvx)}{q(\rvz_l|\rvx)}\\
&= \frac{q(\rvz_l|\rvz_{l-1})q(\rvz_{l-1}|\rvx)}{q(\rvz_l|\rvx)} \\
& = \mathcal{N}(\rvz_{l-1} | \mu(\rvz_l, \rvx), \tilde{\sigma}^2_l\text{I}).
\end{align}

Therefore, the total variational posterior distribution can be expressed in the following form:
\begin{align}
    q_{\phi}(\rvz|\rvx) &= q_{\phi}(\rvz_1|\rvx)\prod_{l=2}^L q_{\phi}(\rvz_l|\rvz_{l-1}) \\
    & = q_{\phi}(\rvz_1|\rvx)\prod_{l=2}^L \frac{q(\rvz_{l-1} |\rvz_l, \rvx)q(\rvz_l|\rvx)}{q(\rvz_{l-1}|\rvx)}\\
    & = q_{\phi}(\rvz_1|\rvx)  \frac{\prod_{l=2}^L q(\rvz_l|\rvx)}{\prod_{l=1}^{L-1} q(\rvz_{l}|\rvx)}\prod_{l=2}^L q(\rvz_{l-1} |\rvz_l, \rvx)\\
    % & = q_{\phi}(\rvz_1|\rvx) \frac{q(\rvz_L|\rvx)}{ q(\rvz_{1}|\rvx)}\prod_{l=1}^{L-1} q(\rvz_{l} |\rvz_{l+1}, \rvx)\\
    & = q(\rvz_L|\rvx)\prod_{l=1}^{L-1} q(\rvz_{l} |\rvz_{l+1}, \rvx).
\end{align}

Now, we can plug this into ELBO objective (Eq.~\ref{eq:intro_bottom_up_elbo_2}):
\begin{align}
    \mathcal{L}(\rvx, \theta, \phi) & =\E_{q_{\phi}(\rvz_{1}|\rvx)}\log p_{\theta}(\rvx|\rvz_{1}) \label{eq:intro_dgm_elbo_1}\\
    &\quad\quad-\KL{ q_{\phi}(\rvz_L|\rvx)}{p_{\theta}(\rvz_L)}\label{eq:intro_dgm_elbo_2}\\
    &\quad\quad- \sum_{l=1}^{L-1} \E_{q_{\phi}(\rvz_{l+1}|\rvx)} \KL{q_{\phi}(\rvz_l|\rvz_{l+1}, \rvx)}{p_{\theta}(\rvz_l|\rvz_{l+1})}. \label{eq:intro_dgm_elbo_3}
\end{align}

Let us consider the last term in Eq.~\ref{eq:intro_dgm_elbo_2} and note that both distributions are Gaussian. Furthermore, the common assumption is that both distributions have the same diagonal variance $\sigma_{q_l}$I. Then, the KL term can be written as:
\begin{equation}\label{eq:intro_diff_kl}
    \KL{q_{\phi}(\rvz_l|\rvz_{l+1}, \rvx)}{p_{\theta}(\rvz_l|\rvz_{l+1})} = \frac 12 \frac{\|\mu_q(\rvz_{l+1}, \rvx) - \mu_p(\rvz_{l+1})\|^2}{\sigma_{q_l}^2}.
\end{equation}
\anna{Comment about parameter sharing and summarized the latent variable view}

% \paragraph{Score Matching}
\paragraph{Score Matching View on Diffusion Models}
There is a close connection between Diffusion models discussed in this section and Score-based models.  We can see that by rewriting the objetive function. 
To get a score-matching objective, let us consider the multivariate Gaussian and compute it's score:
\begin{align}
    p(\rvx) &= \mathcal{N}(\rvx|\mu, \sigma^2I),\\
    \nabla_{\rvx}\log p(\rvx) &= \nabla_{\rvx}\left(-\frac k2 \log(2\pi\sigma^2) - \frac12 \frac{\|\rvx - \mu\|^2}{\sigma^2}\right) = \frac{\mu - \rvx}{\sigma^2}. 
\end{align}
Notice that now, we can express the mean as a function of the score for a given value of $\rvx$: $ \mu  = \sigma^2 \nabla_{\rvx}\log p(\rvx) + \rvx$. Thus we can use this to re-write Eq.~\ref{eq:intro_diff_kl} as follows:

\begin{align}
    &\KL{q_{\phi}(\rvz_l|\rvz_{l+1}, \rvx)}{p_{\theta}(\rvz_l|\rvz_{l+1})}, \\
    &\quad\quad= \frac 12 \sigma_{q_l}^2 \|\nabla_{\rvz_l}\log q_{\phi}(\rvz_l|\rvz_{l+1}, \rvx)  - \nabla_{\rvz_l}\log p_{\theta}(\rvz_l|\rvz_{l+1})\|^2
\end{align}

Therefore, minimizing the KL divergence as each stochastic layer corresponds to minimizing the Score Matching objective (Eq.\ref{eq:score_divergence}), scaled by the variance. 

\paragraph{Denoising Diffusion Models}
Lastly, we discuss how diffusion model can be seen as denoising models. We showed that the simplified variational posterior defines each latent variable as a noisy version of the input. The amount of added noise increases a we consider stochastic layers further from the input. 
Generative model, on the other hands, learn the process that goes in the opposite direction. In that sense, it is trained to remove the noise added to the point by the variational posterior. 

If we consider the full generative model, it starts from the standard Gaussian distribution $p(\rvz_L)$ runs a Markov process that terminates at datapoint $\rvz$. This first step clearly does not have any "signal" to it, therefore, generative model cannot be \textit{purely} denoising, it should start by generating the signal. However, as the process gets closer the input, the model should switch to a more "denoising" mode. 

Usually, all the conditional distributions are parametrized by the shared Neural Network which should both work as a generator that creates the signal and as a denoiser that removes the gaussian noise. These two modes of operation can be seen as orthogonal and, furthermore, we mostly happen at the different parts of the model, which motivate the next research question.

\begin{quote}
\marginnote[0\baselineskip]{\normalsize Research Question 3}
	How to decouple denoising and generative abilities of the diffusion generative models
\end{quote}
This research question is addressed in Chapter~\ref{chap:daed}. We empirically study the how generative diffusion model behaves at different stages of the denoising process and propose to decouple two functions. We show that the proposed changes improve model performance as well as out-of-distribution denoising abilities. 

\subsection{Is Variational Posterior Distribution an Encoder?}
Apart from being generative models with the ability to estimate the density and sample from the learned distribution, latent variable generative models may be interesting in their ability to learn representations of data. 
In this regard, it is common to think of the variational posterior as an \textit{encoder}. The mean of the variational posterior or sample from it is then treated as a (usually lower dimensional) representation of the corresponding datapoint. 
However, there is nothing in the training objective function which enforces the learned latent representation to be useful, for example, for downstream application. 
This does not mean that learned representations are useless, but rather that we cannot argue that they are necessarily useful. 
% Furthermore, usefulness is a very vague term. 
% In this section, we discuss several directions in which representation can be useful. 


\paragraph{Adversarial Robustness}
One property, that we are interested data representations to have is robustness to adversarial attacks. 
Adversarial attacks are defined as small perturbations to the in-distribution data points, which result in unexpected model behavior. 
In application to latent representation, we define an \textit{attacked} data point as $\rvx_a = \rvx + \varepsilon,\,\|\varepsilon\|\approx 0$, such that variational posterior $q_{\phi}(\rvz|\rvx_a)$ is very different from $q_{\phi}(\rvz|\rvx)$. 
This means that two very similar datapoints have very different representations.

This motivates to study the latent variable models ability to counteract adversarial attacks and to pose the next research question.
\begin{quote}
\marginnote[0\baselineskip]{\normalsize Research Question 4}
	How  to make latent representations robust to adversarial attacks?
\end{quote}
This research question is addressed in Chapter~\ref{chap:adv_att}. Previous works proposed altering the architecture or training objective to improve the model robustness. In this work, instead, we propose to utilize the decoder to "move" the latent representation closer to the true posterior distribution. 

\paragraph{Equivariant Latent Representation}
Apart from generally useful properties like robustness discussed above, there are properties that might be useful for specific downstream task. 
For example, downstream tasks can benefit from utilizing symmetries presented in the data. 

\begin{marginfigure}
\begin{tikzpicture}
% Define nodes
% \node (x) at (0,0) [circle, draw, fill=blue!20] {$x$};

% this is x
\node[draw, thick, minimum size=1cm] (x) at (0,0) {
    \tikz \node[star, star points=5, star point ratio=2.5, fill=RoyalBlue!60, inner sep=1pt] {};
};

% This is rotated x
\node[draw, thick, minimum size=1cm, rotate=45] (rot_x) at (0,-2.5) {
    \tikz \node[star, star points=5, star point ratio=2.5, fill=RoyalBlue!60, inner sep=1pt, rotate=45] {};
};

% This if f(x)
\node[draw, thick, star, star points=5, star point ratio=2.1, fill=ForestGreen!30, minimum size=0.6cm,  transform shape, scale=0.7] (fx) at (2.5,0) {
    \tikz \node[rectangle, minimum size=0.5cm, fill=white] {};
};

\node[draw, thick, star, star points=5, star point ratio=2.1, fill=ForestGreen!30, minimum size=0.6cm,  transform shape, scale=0.65, rotate=45] (rot_fx) at (2.5,-2.5) {
    \tikz \node[rectangle, minimum size=0.5cm, fill=white] {};
};

% Draw arrows
\draw[->, thick] (x) -- (fx) node[midway, above] {\normalsize$f$};
\draw[->, thick] (x) -- (rot_x) node[midway, left] {\normalsize$T$};
\draw[->, thick] (fx) -- (rot_fx) node[midway, left] {\normalsize$T$};
\draw[->, thick] (rot_x) -- (rot_fx) node[midway, above] {\normalsize$f$};

\end{tikzpicture}
\caption{Example of a function $f$ equivariant to transformation $T$. Applying transformation and then function yields the same result as first applying the function and then the transformation.}\label{fig:intro_equiv}
\end{marginfigure}
One such downstream task is compressed sensing. Here, we are interested in reconstructing a signal (e.g. an image) from a limited set of measurements which can be converted into an optimization problem. In this case, latent variable generative model can be used as a prior over all possible signals and we can solve the optimization problem in the lower dimensional latent space instead. 
The task may have additional complexity if we add rotation ambiguity. Namely, we can assume that the object could have been measured in an unknown orientation. In that case, we can benefit from learning the latent representations that are \textit{equivariant} to rotation. 

We provide a toy example to illustrate a rotation equivariant function in Figure \ref{fig:intro_equiv}. We can first rotate an object and then evaluate the function, or we can first evaluate the function and then rotate the output. If we get the same result in both cases, the function is called equivariant to rotation. 

If the latent representation is equivariant, then the same object in different orientations will have the same latent code up to rotation. This can help solving compressed sensing task. 
This motivates the next research question studied in this thesis. 

\begin{quote}
\marginnote[0\baselineskip]{\normalsize Research Question 5}
	How to learn equivariant latent representations using VAE?
\end{quote}
This research question is addressed in Chapter~\ref{chap:eqvae}, where we introduce the equivariant VAE and use it as a prior for the compressed sensing task. 


% \newpage
\section{Dissertation Structure}
The thesis is split into two parts, part~\ref{part:1} focuses on the improved density estimation. Part~\ref{part:2}, on the other hand, takes a closer look at the properties of the learned data representations. 

Each chapter in this dissertation is based on one or more published papers.
We have chosen to keep the paper consistent with the original publication, which allows reading each chapter as an independent unit. 
Each paper is accompanied with Appendix, which is located at the end of the dissertation.


% In Table \ref{tab:papers_and_contributions} we provide the matrix of contributions, so that the reader interested in a particular aspect can refer to the 
% \begin{table}[!ht]
% 	\caption[][\baselineskip]{Contributions to Different Aspects of Generative Modeling .}
% 	\label{tab:papers_and_contributions}
% 	\begin{center}
% %		\resizebox{\textwidth}{!}{
% 			\begin{tabular}{ll|cccc}
% 				\toprule
% 				& & Learned   & Downstream & Latent                & Architecture \\
% 				& & Prior & Application  & Space Properties &                      \\ \midrule
% 				\multirow{3}{*}{\STAB{\rotatebox[origin=c]{90}{Part 1}}}
% 				& Chapter \ref{chap:boovae} & $\checkmark$ & $\checkmark$ & \\
% 				& Chapter \ref{chap:dvp} & $\checkmark$ & & & $\checkmark$\\ 
%                     & Chapter \ref{chap:daed} &  & & $\checkmark$ &\\\midrule
% 				\multirow{2}{*}{\STAB{\rotatebox[origin=c]{90}{Part 2}}}
% 				& Chapter \ref{chap:adv_att} & &$\checkmark$ & $\checkmark$ &\\
%                     & Chapter \ref{chap:eqvae}&  &$\checkmark$ & $\checkmark$ & $\checkmark$\\
% 				 \midrule
% 				\bottomrule
% 				% \end{tabularx}
% 			\end{tabular}
% %		}
% 	\end{center}
% 	\vspace*{\baselineskip}
% \end{table}
%\section{Research Questions}
%
%\begin{enumerate}
%\item Approximate optimal prior improves the VAE performance. How to adjust it to be applicable to continual learning setting and be scalable to deep hierarchical VAEs?
%\item How to make sure that latent representation exhibit predictable performance under certain input transformations, such as specifically constrcuted noise or action of the symmetry group?
%\item  What are the banifits of using VAEs as generative prior in continual learning and compressed sensing
%\item  Can we decouple denoising and generative functions of the diffusion latent space?
%\item  How to improve latent space utilization in deep hierarchical VAEs?
%\end{enumerate}

% \paragraph{How to read this dissertation}


