\chapter{Introduction}\label{chap:intro}

% \begin{quote}
% \normalsize\itshape
% \begin{flushright}
% \foreignlanguage{russian}{А вопросы… Вопросы не знают ответа —}\\
% \foreignlanguage{russian}{Налетят, разожгут и умчатся, как корь.} \\
% \foreignlanguage{russian}{Саша Черный} \\ \vskip 10pt
%  And the questions...  The questions lack answers, still missing:\\
%  They'll come and they'll burn, fade like measles, unkind.\\
%  Sasha Chorny
% \end{flushright}
% \end{quote}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
% Gneral Introduction, some talkson how deep generative mdoels are cool goes here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
In recent years, deep learning has seen significant advances, particularly in the field of generative models. Two of the most successful applications of Deep Generative Models (DGMs) are natural language processing and computer vision. In natural language processing, Large Language Models (LLMs), which usually constitute autoregressive generative models~\citep{graves2013generating} with a transformer-based architecture~\citep{vaswani2017attention}, have demonstrated impressive text generation capabilities~\citep{brown2020language,chowdhery2023palm}. They have produced long and coherent texts across various contexts and gained popularity in the form of chat bots~\citep{achiam2023gpt}. In computer vision, diffusion models are achieving impressive results in generating photorealistic images~\citep{dhariwal2021diffusion} and videos~\citep{ho2022video}.

% Bayesian inference and human learning \cite{xu2007word}

Practically, modeling the probability distribution of the data offers two main functionalities to the underlying model: sampling and likelihood estimates. A lot of attention is paid to the sampling functionality, as it can be used to produce completely novel datapoints, e.g. images that look like real ones but that were not observed by the model during training. Furthermore, conditional sampling from the distribution can be used to provide forecast or prediction in the situation when only part of the variables is observed. Access to the likelihood, if present, might also offer many benefits. This includes, but is not limited to, the estimation of uncertainty and the detection of out-of-distribution \cite{havtorn2021hierarchical, kadavath2022language}, as well as handling the missing data \cite{mattei2019miwae} and anomalies \citep{an2015variational}.

Latent Variable Models (LVMs) is a specific type of generative models, where the expressivity of the model is increased by introducing hidden (latent) variables.
These variables are meant to capture the underlying structures or factors that are not directly observed but influence the data generation process.
During training, Latent Variable Models are trained to perform density estimation of the data and at the same time to
approximate the conditional distribution of the unobserved latent variables given the data. 
Thus, LVMs can produce realistic samples and enable additional functionality of representation learning. 

Variational Autoencoder and Diffusion Models are the two most popular examples of latent variable models. Both models use neural network parametrizes generative process to produce realistic data sample from the latent variables. 
These models differ in the way latent variables are defined with VAEs offering a more flexible approach of an encoder parametrized by a neural network, while diffusion model consider a non-trainable diffusion process that autoregressively transforms input data into noise. 


% Key examples of latent variable generative models include variational autoencoders (VAEs), which use deep neural networks to encode and decode data through a probabilistic framework, and generative adversarial networks (GANs), where a generator learns to produce realistic data samples while a discriminator distinguishes real from fake samples. These models have found applications in image synthesis, text generation, and unsupervised learning.



\section{Research Goal}
Despite enormous successes, many challenges remain in the field of deep generative modeling~\citep{manduchi2024challenges} with latent variable models not being an exception. 
Identifying and addressing these challenges is an overarching research goal of this thesis, which we formulate below.

\begin{quote}
\normalsize\itshape \noindent 
Identify and address weaknesses of latent variable generative models.
\marginnote[-0.1\baselineskip]{\itshape Research Goal}
\end{quote}

We consider two general aspects in which deep LVMs can be improved: density estimation and representation learning. In the former aspect, our aim was to improve the likelihood estimation. In the latter case, we focus on studying and improving the learned representations. 

\paragraph{Density estimation}
The first aspect concerns the choice of the probabilistic model that improves the density estimation performance of the model. 
First, we consider a dynamic scenario of continual learning for VAEs. 
Changes to the probabilistic model that we propose allow one to expand model capacity and combat catastrophic forgetting that is common in continual learning. 
We then switch to a static training of models with hierarchical structure, where we consider both the VAE and the diffusion model.
% We consider both dynamic scenario continual learning and static training of VAEs and diffusion models. 

\paragraph{Representation Learning}
The second aspect focuses on the properties of the latent representations learned by the models.
This is especially important for the downstream applications of the latent variable generative models. 
In this regard, we study the robustness of latent representations to adversarial attacks and introduce the equivariant VAE, which preserves the symmetries of the data.


\section{Dissertation Structure}
First we provide an extensive background on Deep Generative Modeling in general and latent variable model specifically in Chapter \ref{ch:background}. We focus on the concepts and techniques used throughout the thesis and use this detailed context to formulate research questions addressed by each of the consecutive chapters. 

Each chapter in the following is based on one or more published papers. We divide them into two parts. Part~\ref{part:1} focuses on improved density estimation. Part~\ref{part:2}, on the other hand, takes a closer look at the properties of the learned data representations. 

In total, the thesis is based on four $A^*$ conference publications, one journal article, and one workshop article.  
We have chosen to keep the papers consistent with the original publication, which allows reading each chapter as an independent unit. 
Each paper is accompanied by an appendix which is located at the end of the dissertation.
In Table \ref{tab:papers_and_contributions} we arrange the articles according to the topic, so that the reader interested in a particular aspect can refer to the corresponding chapter.

\begin{table}[!ht]
	\caption[][\baselineskip]{Topic and year of the publications for each chapter.}
	\label{tab:papers_and_contributions}
	\begin{center}
%		\resizebox{\textwidth}{!}{
			\begin{tabular}{ll|cccc}
				\toprule
				 & & Chapter & Year & Topic \\
                 \midrule
				\multirow{3}{*}{\STAB{\rotatebox[origin=c]{90}{Part \ref{part:1}}}} &
                \multirow{3}{*}{Density Estimation}
				% & Ch. \ref{chap:boovae} & $\checkmark$ & $\checkmark$ & \\
				% & Ch. \ref{chap:dvp} & $\checkmark$ & & & $\checkmark$\\ 
    %                 & Ch. \ref{chap:daed} &  & & $\checkmark$ &\\\midrule
				% \multirow{2}{*}{\STAB{\rotatebox[origin=c]{90}{Part 2}}}
				% & Ch. \ref{chap:adv_att} & &$\checkmark$ & $\checkmark$ &\\
    %                 & Ch. \ref{chap:eqvae}&  &$\checkmark$ & $\checkmark$ & $\checkmark$\\
    & Ch. \ref{chap:boovae} & 2021 & Continual Learning\\
    && Ch. \ref{chap:dvp} & 2024   & Hierarchical VAEs\\ 
    && Ch. \ref{chap:daed} & 2022  & Diffusion Models \\\midrule
\multirow{2}{*}{\STAB{\rotatebox[origin=c]{90}{Part \ref{part:2}}}} &
    % \multirow{2}{*}{Latent Space Properties} 
    Latent Space
    & Ch. \ref{chap:adv_att} & 2021, 2022 & Adversarial Robustness\\
    &Properties& Ch. \ref{chap:eqvae}&   2022 & Equivariance\\
    \midrule
    \bottomrule
				% \end{tabularx}
			\end{tabular}
%		}
	\end{center}
	\vspace*{4\baselineskip}
\end{table}

\section{List of Publications}
The following publications form the basis of this thesis:

\begin{itemize}[leftmargin=15pt, rightmargin=10pt]
% \setlength{\itemindent}{0pt}
% \setlength{\leftmargin}{2cm}
% \setlength{\rightmargin}{2cm}
% \setlength\itemsep{15pt}
% \item {[1]}
    \item \textbf{Anna Kuzina}\footnote[1]{Shared first authorship}, Evgenii Egorov\footnotemark[1], Evgeny Burnaev. \\
    BooVAE: Boosting approach for continual learning of VAE. \\
    \textit{Advances in Neural Information Processing Systems, 2021.}
    \item  \textbf{Anna Kuzina},  Jakub M Tomczak. \\ 
    Hierarchical VAE with a Diffusion-based VampPrior.\\
    \textit{Transactions on Machine Learning Research, 2024.}
    \item \textbf{Anna Kuzina}\footnotemark[1], Kamil Deja\footnotemark[1], Tomasz Trzcinski, Jakub M Tomczak. \\
    On analyzing generative and denoising capabilities of diffusion-based deep generative models. \\
    \textit{Advances in Neural Information Processing Systems, 2022.}
    \item \textbf{Anna Kuzina}, Max Welling, Jakub M Tomczak.  \\
    Diagnosing vulnerability of variational auto-encoders to adversarial attacks. \\
    \textit{International Conference on Learning Representations. Workshop on Robust Machine Learning, 2021}
    \item \textbf{Anna Kuzina}, Max Welling, Jakub M Tomczak.  \\
    Alleviating adversarial attacks on variational autoencoders with MCMC. \\
    \textit{Advances in Neural Information Processing Systems, 2022.}
    \item \textbf{Anna Kuzina}, Kumar Pratik, Fabio V Massoli, Arash Behboodi.\\ 
    Equivariant priors for compressed sensing with unknown orientation. \\
    \textit{International Conference on Machine Learning, 2022.}
\end{itemize}

As a first author, I have contributed to the publications listed above in all aspects, namely, formulating the problem, designing the solution, running the experiments, and writing the text. 
Two articles were written in an equal contribution with the other co-author.  
In these articles, the formulation of problems and solutions was developed in collaboration. Furthermore, we contributed equally to running experiments and writing the text.

Below, I list the papers that are not included in this thesis, but that I contributed to during my PhD. 
\begin{itemize}[leftmargin=15pt, rightmargin=10pt]
    \item David W Romero, \textbf{Anna Kuzina}, Erik J Bekkers, Jakub M Tomczak, Mark Hoogendoorn. \\
    CKConv: Continuous Kernel Convolution For Sequential Data. \\
    \textit{International Conference on Learning Representations, 2022.}
     \item Michał Zając, Kamil Deja, \textbf{Anna Kuzina}, Jakub M Tomczak, Tomasz Trzciński, Florian Shkurti, and Piotr Miłoś. \\
     Exploring continual learning of diffusion models. \\
     \textit{Conference on Computer Vision and Pattern Recognition. Workshop on Continual Learning in Computer Vision, 2023}
    \item \textbf{Anna Kuzina}\footnotemark[1], Haotian Chen\footnotemark[1], Babak Esmaeili, Jakub M Tomczak. \\
    Variational Stochastic Gradient Descent for Deep Neural Networks. \\
    \textit{International Conference on Machine Learning. Workshop on Advancing Neural Network Training, 2024.}
    % \item Yicheng Chen\footnotemark[1], \textbf{Anna Kuzina}\footnotemark[1], ... \\
    % Edgified model: scalable efficient transformer for intricate atomic interactions. \\
    % \textit{Pre-print, 2025.}
\end{itemize}

% \icmlauthor{Guillem Simeon}{smt}
% \icmlauthor{Lixue Cheng}{ai4s}
% \icmlauthor{Yatao Li}{ai4s}
% \icmlauthor{Jean Helie}{ai4s}
% \icmlauthor{Hannes Schulz}{ai4s}
% \icmlauthor{Jiaming Shen}{ai4s,thu}
% \icmlauthor{Gregor Simm}{ai4s}
% \icmlauthor{Zun Wang}{ai4s}
% \icmlauthor{Chi Chen}{smt}
% \icmlauthor{Xixan Liu}{ai4s,fdu}
% \icmlauthor{Yu Zhu}{ai4s,fdu}
% \icmlauthor{Hongxia Hao}{ai4s}
% \icmlauthor{Jielan Li}{ai4s}
% \icmlauthor{Han Yang}{ai4s}
% \icmlauthor{Piero Gasparotto}{smt}
% \icmlauthor{Ziheng Lu}{ai4s}
% \icmlauthor{Lixin Sun}{ai4s}