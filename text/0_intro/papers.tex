\chapter*{List of Publications}\label{chapter:papers}
\addcontentsline{toc}{chapter}{\nameref{chapter:papers}}

The following publications form the basis of this thesis:
\begin{itemize}[leftmargin=20pt, rightmargin=10pt]
% \setlength{\itemindent}{0pt}
% \setlength{\leftmargin}{2cm}
% \setlength{\rightmargin}{2cm}
% \setlength\itemsep{15pt}
    \item \textbf{Anna Kuzina}\footnote[1]{Shared first authorship}, Evgenii Egorov\footnotemark[1], Evgeny Burnaev. \\
    BooVAE: Boosting approach for continual learning of VAE. \\
    \textit{Advances in Neural Information Processing Systems, 2021.}
    \item \textbf{Anna Kuzina},  Jakub M Tomczak. \\ 
    Hierarchical VAE with a Diffusion-based VampPrior.\\
    \textit{Transactions on Machine Learning Research, 2024.}
    \item \textbf{Anna Kuzina}, Kumar Pratik, Fabio V Massoli, Arash Behboodi.\\ 
    Equivariant priors for compressed sensing with unknown orientation. \\
    \textit{International Conference on Machine Learning, 2022.}
    \item \textbf{Anna Kuzina}, Max Welling, Jakub M Tomczak.  \\
    Diagnosing vulnerability of variational auto-encoders to adversarial attacks. \\
    \textit{International Conference on Learning Representations. Workshop on Robust Machine Learning, 2021}
    \item \textbf{Anna Kuzina}, Max Welling, Jakub M Tomczak.  \\
    Alleviating adversarial attacks on variational autoencoders with MCMC. \\
    \textit{Advances in Neural Information Processing Systems, 2022.}
    \item \textbf{Anna Kuzina}\footnotemark[1], Kamil Deja\footnotemark[1], Tomasz Trzcinski, Jakub M Tomczak. \\
    On analyzing generative and denoising capabilities of diffusion-based deep generative models. \\
    \textit{Advances in Neural Information Processing Systems, 2022.}

\end{itemize}

As a first author, I have contributed to the publications listed above in all aspects, namely, formulating the problem, designing the solution, running the experiments, and writing the text. 
Two articles were written in an equal contribution with the other coauthors.  
In these articles, the formulation of problems and solutions was developed in collaboration. Furthermore, we contributed equally to running experiments and writing text.
% these coauthors also contributed substantially to the 
% conception of the ideas or experiments.
% Jakub Tomczak, Max Welling, and Arash Behboodi provided supervision, guidance, insight, and technical advice.

%In the two papers were first authorship is shared me and the other first author contributed equally to the 
% In "MDP Homomorphic Networks: Group Symmetries in Reinforcement Learning." Daniel Worrall proposed the Symmetrizer, which was implemented by us jointly.
% In "Contrastive Learning of Structured World Models." Thomas Kipf
% contributed in all aspects. I provided reinforcement learning insights,
% proposed the use of action factorization and the evaluation method,
% and ran baseline experiments. Figures and tables reproduced with permission. In "Geometric and Physical Quantities improve E(3) Equivariant Message Passing.", Johannes Brandstetter, Rob Hesselink, and Erik
% Bekkers contributed in all aspects. I provided the original architectural
% idea and took an advisory role. Figures reproduced with permission.

% Below, we list other papers that are not included in this thesis, but that I have contributed to during my PhD.
The papers below are not included in this thesis, but I have contributed to the during my PhD.
\begin{itemize}[leftmargin=20pt, rightmargin=10pt]
    \item David W Romero, \textbf{Anna Kuzina}, Erik J Bekkers, Jakub M Tomczak, Mark Hoogendoorn. \\
    CKConv: Continuous Kernel Convolution For Sequential Data. \\
    \textit{International Conference on Learning Representations, 2022.}
     \item Michał Zając, Kamil Deja, \textbf{Anna Kuzina}, Jakub M Tomczak, Tomasz Trzciński, Florian Shkurti, and Piotr Miłoś. \\
     Exploring continual learning of diffusion models. \\
     \textit{Conference on Computer Vision and Pattern Recognition. Workshop on Continual Learning in Computer Vision, 2023}
    \item \textbf{Anna Kuzina}\footnotemark[1], Haotian Chen\footnotemark[1], Babak Esmaeili, Jakub M Tomczak. \\
    Variational Stochastic Gradient Descent for Deep Neural Networks. \\
    \textit{International Conference on Machine Learning. Workshop on Advancing Neural Network Training, 2024.}
    \item \textbf{Anna Kuzina}\footnotemark[1], \footnotemark[1], . \\
    Edgified model: scalable efficient transformer for intricate atomic interactions. \\
    \textit{Pre-print, 2025.}
\end{itemize}